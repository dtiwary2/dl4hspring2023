{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06fcd6e6",
   "metadata": {},
   "source": [
    "CS598 DL4H - Spring 2023\n",
    "Dhiraj Tiwary\n",
    "Dtiwary2@illinois.edu\n",
    "Group ID: 116\n",
    "Paper ID: 45\n",
    "Presentation link: https://youtu.be/k6YI1bnn-fY\n",
    "Code link: https://github.com/dtiwary2/dl4hspring2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9775dc9",
   "metadata": {},
   "source": [
    "MIMIC - data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabfcd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# INSERT FILE DESCRIPTION\n",
    "\n",
    "\"\"\"\n",
    "Util functions to run Model. Includes Data loading, etc...\n",
    "\"\"\"\n",
    "import os\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import datetime as dt\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def _compute_last_target_id(df: pd.DataFrame, time_col: str = \"intime\", mode: str = \"max\") -> pd.DataFrame:\n",
    "    \"\"\"Identify last ids given df according to time given by time_col column. Mode determines min or max.\"\"\"\n",
    "    if mode == \"max\":\n",
    "        time = df[time_col].max()\n",
    "    elif mode == \"min\":\n",
    "        time = df[time_col].min()\n",
    "    else:\n",
    "        raise ValueError(\"mode must be one of ['min', 'max']. Got {}\".format(mode))\n",
    "\n",
    "    last_ids = df[df[time_col] == time]\n",
    "\n",
    "    return last_ids\n",
    "\n",
    "\n",
    "def _rows_are_in(df1: pd.DataFrame, df2: pd.DataFrame, matching_columns: Union[List[str], str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Checks if values present in row of df1 exist for all columns in df2. Note that this does not necessarily mean\n",
    "    the whole row of df1 is in df2, but is good enough for application.\n",
    "\n",
    "    Returns: array of indices indicating the relevant rows of df1.\n",
    "    \"\"\"\n",
    "    if isinstance(matching_columns, str):\n",
    "        matching_columns = [matching_columns]\n",
    "\n",
    "    # Iterate through each column\n",
    "    matching_ids = np.ones(df1.shape[0])\n",
    "    for col in tqdm(matching_columns):\n",
    "        col_matching = df1[col].isin(df2[col].values).values  # where df1 col is subset of df2 col\n",
    "        matching_ids = np.logical_and(matching_ids, col_matching)  # match with columns already looked at\n",
    "\n",
    "    return matching_ids\n",
    "\n",
    "\n",
    "def _compute_second_transfer_info(df: pd.DataFrame, time_col, target_cols):\n",
    "    \"\"\"\n",
    "    Given transfer data for a unique id, compute the second transfer as given by time_col.\n",
    "\n",
    "    return: pd.Series with corresponding second transfer info.\n",
    "    \"\"\"\n",
    "    time_info = df[time_col]\n",
    "    second_transfer_time = time_info[time_info != time_info.min()].min()\n",
    "\n",
    "    # Identify second transfer info - can be empty, unique, or repeated instances\n",
    "    second_transfer = df[df[time_col] == second_transfer_time]\n",
    "\n",
    "    if second_transfer.empty:\n",
    "        output = [df.name, df[\"hadm_id\"].iloc[0], df[\"transfer_id\"].iloc[0]] + [np.nan] * (len(target_cols) - 3)\n",
    "        return pd.Series(data=output, index=target_cols)\n",
    "\n",
    "    elif second_transfer.shape[0] == 1:\n",
    "        return pd.Series(data=second_transfer.squeeze().values, index=target_cols)\n",
    "\n",
    "    else:  # There should be NONE\n",
    "        print(second_transfer)\n",
    "        raise ValueError(\"Something's gone wrong! No expected repeated second transfers with the same time.\")\n",
    "\n",
    "\n",
    "def convert_columns_to_dt(df: pd.DataFrame, columns: Union[str, List[str]]):\n",
    "    \"\"\"Convert columns of dataframe to datetime format, as per given\"\"\"\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "\n",
    "    for col in columns:\n",
    "        df[col] = pd.to_datetime(df.loc[:, col].values)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def subsetted_by(df1: pd.DataFrame, df2: pd.DataFrame, matching_columns: Union[List[str], str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subset df1 based on matching_columns, according to values existing in df2.\n",
    "\n",
    "    Returns: pd.DataFrame subset of df1 for which rows are a subset of df2\n",
    "    \"\"\"\n",
    "\n",
    "    return df1.iloc[_rows_are_in(df1, df2, matching_columns), :]\n",
    "\n",
    "\n",
    "def endpoint_target_ids(df: pd.DataFrame, identifier: str, time_col: str = \"intime\", mode: str = \"max\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given identifier target (\"id\"), compute the endpoint associated with time column.\n",
    "\n",
    "    Returns: pd.DataFrame with ids and associated endpoint information.\n",
    "    \"\"\"\n",
    "    last_ids = df.groupby(identifier, as_index=False).progress_apply(\n",
    "        lambda x: _compute_last_target_id(x, time_col=time_col, mode=mode))\n",
    "\n",
    "    return last_ids.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def compute_second_transfer(df: pd.DataFrame, identifier: str, time_col: str, target_cols: pd.Index) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given transfer data represented by unique identifier (\"id\"), compute the second transfer of the admission.\n",
    "    Second Transfer defined as second present intime in the date (if multiple, this is flagged). If there are\n",
    "    no transfers after, then return np.nan. target_cols is the target information columns.\n",
    "\n",
    "    This function checks the second transfer intime is after outtime of first transfer record.\n",
    "\n",
    "    Returns: pd.DataFrame with id and associated second transfer information (intime/outtime, unit, etc...)\n",
    "    \"\"\"\n",
    "    second_transfer_info = df.groupby(identifier, as_index=False).progress_apply(\n",
    "        lambda x: _compute_second_transfer_info(x, time_col, target_cols))\n",
    "\n",
    "    return second_transfer_info.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _has_many_nas(df: pd.DataFrame, targets: Union[List[str], str], min_count: int, min_frac: float) -> bool:\n",
    "    \"\"\"\n",
    "    For a given admission/stay with corresponding vital sign information, return boolean indicating whether low\n",
    "    missingness conditions are satisfied. These are:\n",
    "    a) At least min_count observations.\n",
    "    b) Proportion of missing values smaller than min_frac for ALL targets.\n",
    "\n",
    "    returns: boolean indicating admission should be kept.\n",
    "    \"\"\"\n",
    "    if isinstance(targets, str):\n",
    "        targets = [targets]\n",
    "\n",
    "    has_minimum_counts = df.shape[0] > min_count\n",
    "    has_less_NA_than_frac = df[targets].isna().sum() <= min_frac * df.shape[0]\n",
    "\n",
    "    return has_minimum_counts and has_less_NA_than_frac.all()\n",
    "\n",
    "\n",
    "def remove_adms_high_missingness(df: pd.DataFrame, targets: Union[List[str], str],\n",
    "                                 identifier: str, min_count: int, min_frac: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given vital sign data, remove admissions with too little information. This is defined as either:\n",
    "    a) Number of observations smaller than allowed min_count.\n",
    "    b) Proportion of missing values in ANY of the targets is higher than min_frac.\n",
    "\n",
    "    Returns: pd.DataFrame - Vital sign data of the same type, except only admissions with enough information are kept.\n",
    "    \"\"\"\n",
    "    output = df.groupby(identifier, as_index=False).filter(\n",
    "        lambda x: _has_many_nas(x, targets, min_count, min_frac))\n",
    "\n",
    "    return output.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _resample_adm(df: pd.DataFrame, rule: str, time_id: str,\n",
    "                  time_vars: Union[List[str], str], static_vars: Union[List[str], str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For a particular stay with vital sign data as per df, resample trajectory data (subsetted to time_vars),\n",
    "    according to index given by time_to_end and as defined by rule. It is important that time_to_end decreases\n",
    "    throughout admissions and hits 0 at the end - this is for resampling purposes.\n",
    "\n",
    "    Params:\n",
    "    df: pd.Dataframe, containing trajectory and static data for each admission.\n",
    "    rule: str, indicates the resampling rule (to be fed to pd.DataFrame.resample())\n",
    "\n",
    "    static_vars is a list of relevant identifier information\n",
    "\n",
    "    returns: Resampled admission data. Furthermore, two more info columns are indicated (chartmax and chartmin).\n",
    "    \"\"\"\n",
    "    if isinstance(time_vars, str):\n",
    "        time_vars = [time_vars]\n",
    "\n",
    "    if isinstance(static_vars, str):\n",
    "        static_vars = [static_vars]\n",
    "\n",
    "    # Add fake observation (with missing values) so that resampling starts at end of admission\n",
    "    df_inter = df[time_vars + [\"time_to_end\"]]\n",
    "    df_inter = df_inter.append(pd.Series(data=[np.nan] * len(time_vars) + [dt.timedelta(seconds=0)],\n",
    "                                         index=df_inter.columns), ignore_index=True)\n",
    "\n",
    "    # resample on time_to_end axis\n",
    "    output = df_inter.sort_values(by=\"time_to_end\", ascending=False).resample(\n",
    "        on=\"time_to_end\",\n",
    "        rule=rule, closed=\"left\", label=\"left\").mean()\n",
    "\n",
    "    # Compute static ids manually and add information about max and min time id values\n",
    "    output[static_vars] = df[static_vars].iloc[0, :].values\n",
    "    output[time_id + \"_min\"] = df[time_id].min()\n",
    "    output[time_id + \"_max\"] = df[time_id].max()\n",
    "\n",
    "    # Reset index to obtain resampled values\n",
    "    output.index.name = f\"sampled_time_to_end({rule})\"\n",
    "    output.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def compute_time_to_end(df: pd.DataFrame, id_key: str, time_id: str, end_col: str):\n",
    "    \"\"\"\n",
    "    Compute time to end of admission for a given observation associated with a particular admission id.\n",
    "\n",
    "    df: pd.DataFrame with trajectory information.\n",
    "    id_key: str - column of df representing the unique id admission identifier.\n",
    "    time_id: str - column of df indicating time observations was taken.\n",
    "    end_col: str - column of df indicating, for each observation, the end time of the corresponding admission.\n",
    "\n",
    "    returns: sorted pd.DataFrame with an extra column indicating time to end of admission. This will be used for\n",
    "    resampling.\n",
    "    \"\"\"\n",
    "    df_inter = df.copy()\n",
    "    df_inter[\"time_to_end\"] = df_inter[end_col] - df_inter[time_id]\n",
    "    df_inter.sort_values(by=[id_key, \"time_to_end\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "    return df_inter\n",
    "\n",
    "\n",
    "def conversion_to_block(df: pd.DataFrame, id_key: str, rule: str,\n",
    "                        time_vars: Union[List[str], str], static_vars: Union[List[str], str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given trajectory data over multiple admissions (as specified by id), resample each admission according to time\n",
    "    until the end of the admission. Resampling according to rule and apply to_time_vars.\n",
    "\n",
    "    df: pd.DataFrame containing trajectory and static data.\n",
    "    id_key: str, unique identifier per admission\n",
    "    rule: str, indicates resampling rule (to be fed to pd.DataFrame.resample())\n",
    "    time_vars: list of str, indicates columns of df to be resampled.\n",
    "    static_vars: list of str, indicates columns of df which are static, and therefore not resampled.\n",
    "\n",
    "    return: Dataframe with resampled vital sign data.\n",
    "    \"\"\"\n",
    "    if \"time_to_end\" not in df.columns:\n",
    "        raise ValueError(\"'time_to_end' not found in columns of dataframe. Run 'compute_time_to_end' function first.\")\n",
    "    assert df[id_key].is_monotonic and df.groupby(id_key).apply(\n",
    "        lambda x: x[\"time_to_end\"].is_monotonic_decreasing).all()\n",
    "\n",
    "    # Resample admission according to time_to_end\n",
    "    output = df.groupby(id_key).progress_apply(lambda x: _resample_adm(x, rule, \"time_to_end\", time_vars, static_vars))\n",
    "\n",
    "    return output.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def convert_to_timedelta(df: pd.DataFrame, *args) -> pd.DataFrame:\n",
    "    \"\"\"Convert all given cols of dataframe to timedelta.\"\"\"\n",
    "    output = df.copy()\n",
    "    for arg in args:\n",
    "        output[arg] = pd.to_timedelta(df.loc[:, arg])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def _check_all_tables_exist(folder_path: str):\n",
    "    \"\"\"TO MOVE TO TEST\"\"\"\n",
    "    try:\n",
    "        assert os.path.exists(folder_path)\n",
    "    except Exception:\n",
    "        raise ValueError(\"Folder path does not exist - Input {}\".format(folder_path))\n",
    "\n",
    "\n",
    "def select_death_icu_acute(df, admissions_df, timedt):\n",
    "    \"\"\"\n",
    "    Identify outcomes based on severity within the consequent 12 hours:\n",
    "    a) Death\n",
    "    b) Entry to ICU Careunit\n",
    "    c) Transfer to hospital ward\n",
    "    d) Discharge\n",
    "\n",
    "    Params:\n",
    "    - df - transfers dataframe corresponding to a particular admission.\n",
    "    - timedt - datetime timedelta indicating range window of prediction\n",
    "\n",
    "    Returns categorical encoding of the corresponding admission.\n",
    "    Else returns 0,0,0,0 if a mistake is found.\n",
    "    \"\"\"\n",
    "    # Check admission contains only one such row\n",
    "    assert admissions_df.hadm_id.eq(df.name).sum() <= 1\n",
    "\n",
    "    # Identify Last observed vitals for corresponding admission\n",
    "    hadm_information = admissions_df.query(\"hadm_id==@df.name\").iloc[0, :]\n",
    "    window_start_point = hadm_information.loc[\"outtime\"]\n",
    "\n",
    "    # First check if death exists\n",
    "    hadm_information = admissions_df.query(\"hadm_id==@df.name\")\n",
    "    if not hadm_information.empty and not hadm_information.dod.isna().all():\n",
    "        time_of_death = hadm_information.dod.min()\n",
    "        time_from_start_point = (time_of_death - window_start_point)\n",
    "\n",
    "        # try:\n",
    "        #     assert time_from_vitals >= dt.timedelta(seconds=0)\n",
    "        #\n",
    "        # except AssertionError:\n",
    "        #     return pd.Series(data=[0, 0, 0, 0, time_of_death], index=[\"De\", \"I\", \"W\", \"Di\", \"time\"])\n",
    "\n",
    "        # Check death within time window\n",
    "        if time_from_start_point < timedt:\n",
    "            return pd.Series(data=[1, 0, 0, 0, time_of_death], index=[\"De\", \"I\", \"W\", \"Di\", \"time\"])\n",
    "\n",
    "    # Otherwise, consider other transfers\n",
    "    transfers_within_window = df[df[\"intime\"].between(window_start_point, window_start_point + timedt)]\n",
    "\n",
    "    # Consider icu transfers within window\n",
    "    icu_cond1 = transfers_within_window.careunit.str.contains(\"(?i)ICU\", na=False)  # regex ignore lowercase\n",
    "    icu_cond2 = transfers_within_window.careunit.str.contains(\"(?i)Neuro Stepdown\", na=False)\n",
    "    has_icus = (icu_cond1 | icu_cond2)\n",
    "\n",
    "    if has_icus.sum() > 0:\n",
    "        icu_transfers = transfers_within_window[has_icus]\n",
    "        return pd.Series(data=[0, 1, 0, 0, icu_transfers.intime.min()],\n",
    "                         index=[\"De\", \"I\", \"W\", \"Di\", \"time\"])\n",
    "\n",
    "    # Check to see if discharge has taken\n",
    "    discharges = transfers_within_window.eventtype.str.contains(\"discharge\", na=False)\n",
    "    if discharges.sum() > 0:\n",
    "        return pd.Series(data=[0, 0, 0, 1, transfers_within_window[discharges].intime.min()],\n",
    "                         index=[\"De\", \"I\", \"W\", \"Di\", \"time\"]\n",
    "                         )\n",
    "    else:\n",
    "        return pd.Series(data=[0, 0, 1, 0, transfers_within_window.intime.min()],\n",
    "                         index=[\"De\", \"I\", \"W\", \"Di\", \"time\"]\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e232e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing script for initial ED admission processing.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "####################################################\n",
    "\"\"\"\n",
    "\n",
    "Processing Steps:\n",
    "\n",
    "1. Compute recorded intime and outimes for each ED admission.\n",
    "2. Select admissions with ED as the first admission.\n",
    "3. Remove admissions admitted to special wards, including Partum and Psychiatry. Compute next transfer information.\n",
    "4. Add patient core information.\n",
    "5. Remove admissions without triage information.\n",
    "\n",
    "\n",
    "Other notes.\n",
    "ROW SUBSETTING COULD BE IMPROVED SOMEHOW\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------ // --------------------------------------\n",
    "\"\"\"\n",
    "List of variables used for processing which should be fixed.\n",
    "\n",
    "Data_FD: where the data is saved.\n",
    "SAVE_FD: folder path of interim data saving.\n",
    "ID_COLUMNS: identifiers for admissions, patients and hospital stays.\n",
    "TIME_COLUMNS: list of datetime object columns.\n",
    "WARDS_TO_REMOVE: list of special wards where patients were transferred to and which represent unique populations. This\n",
    "list includes Partum and Psychiatry wards, as well as further ED observations, which generally take place when \n",
    "the hospital is full.\n",
    "AGE_LOWERBOUND: minimum age of patients.\n",
    "PATIENT_INFO: characteristic information for each patient.\n",
    "NEXT_TRANSFER_INFO: list of important info to keep related to the subsequent transfer from ED.\n",
    "\"\"\"\n",
    "DATA_FD = \"data/MIMIC/\"\n",
    "SAVE_FD = DATA_FD + \"interim/\"\n",
    "ID_COLUMNS = [\"subject_id\", \"hadm_id\", \"stay_id\"]\n",
    "TIME_COLUMNS = [\"intime\", \"outtime\", \"charttime\", \"deathtime\"]\n",
    "WARDS_TO_REMOVE = [\"Unknown\", \"Emergency Department\", \"Obstetrics Postpartum\",\n",
    "                   \"Obstetrics Antepartum\", \"Obstetrics (Postpartum & Antepartum)\",\n",
    "                   \"Psychiatry\", \"Labor & Delivery\", \"Observation\", \"Emergency Department Observation\"]\n",
    "AGE_LOWERBOUND = 18\n",
    "PATIENT_INFO = [\"gender\", \"anchor_age\", \"anchor_year\", \"dod\"]\n",
    "NEXT_TRANSFER_INFO = [\"transfer_id\", \"eventtype\", \"careunit\", \"intime\", \"outtime\"]\n",
    "\n",
    "if not os.path.exists(SAVE_FD):\n",
    "    os.makedirs(SAVE_FD)\n",
    "\n",
    "\n",
    "# ------------------------------------- // -------------------------------------\n",
    "def admissions_processing():\n",
    "    \"\"\"\n",
    "    First, Tables are Loaded. We load 4 tables:\n",
    "    \n",
    "    - patients_core: from core/patients filepath. This is a dataframe of patient centralised admission information. \n",
    "    Cohort information for each patient is computed, as well as a unique id which is consistent across all other tables.\n",
    "    \n",
    "    - transfer_core: from core/transfers.csv filepath. This is a dataframe with a list of transfers for each patient.\n",
    "    Includes admissions to ED, but also transfers to wards in the hospital, ICUs, etc...\n",
    "    \n",
    "    - admissions_ed: from ed/edstays.csv filepath. This is a dataframe of patient information indicating relevant\n",
    "    information for any ED admission.\n",
    "    \n",
    "    - triage_ed: from ed/triage.csv filepath. This is a dataframe of patient ED admission indicating triage assessments.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hospital Core\n",
    "    patients_core = pd.read_csv(DATA_FD + \"core/patients.csv\", index_col=None, header=0, low_memory=False)\n",
    "    transfers_core = pd.read_csv(DATA_FD + \"core/transfers.csv\", index_col=None, header=0, low_memory=False,\n",
    "                                 parse_dates=[\"intime\", \"outtime\"])\n",
    "\n",
    "    # ED Admission\n",
    "    admissions_ed = pd.read_csv(DATA_FD + \"ed/edstays.csv\", index_col=None, header=0, low_memory=False,\n",
    "                                parse_dates=[\"intime\", \"outtime\"])\n",
    "    triage_ed = pd.read_csv(DATA_FD + \"ed/triage.csv\", index_col=None, header=0, low_memory=False)\n",
    "\n",
    "    # ------------------------------------- // -------------------------------------\n",
    "    \"\"\"\n",
    "    Process Admission data according to multiple steps.\n",
    "    \n",
    "    Step 1: Remove double admission counts. Select the latest intime. If there are multiple such intimes, \n",
    "    select the last outtime.\n",
    "    Consider only these admissions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute recorded admission intimes and outtimes. Respectively, select latest intime and outtime.\n",
    "    admissions_intime_ed = utils.endpoint_target_ids(admissions_ed, \"subject_id\", \"intime\")\n",
    "    admissions_outtime_ed = utils.endpoint_target_ids(admissions_intime_ed, \"subject_id\", \"outtime\")\n",
    "\n",
    "    admissions_ed_S1 = utils.subsetted_by(admissions_ed, admissions_outtime_ed,\n",
    "                                          [\"stay_id\"])  # last admission information\n",
    "    admissions_ed_S1.to_csv(SAVE_FD + \"admissions_S1.csv\", index=True, header=True)\n",
    "\n",
    "    \"\"\"\n",
    "    Identify those admissions where patients were directly sent to Emergency Department, i.e., the first intime\n",
    "    is in the Emergency Department. \n",
    "    Subset to these admissions.\n",
    "    \"\"\"\n",
    "    # Identify first wards for all admissions to hospital\n",
    "    transfers_first_ward = utils.endpoint_target_ids(transfers_core, \"subject_id\", \"intime\", mode=\"min\")\n",
    "    ed_first_transfer = transfers_first_ward[(transfers_first_ward[\"eventtype\"] == \"ED\") &\n",
    "                                             (transfers_first_ward[\"careunit\"] == \"Emergency Department\")]\n",
    "\n",
    "    # Subset to admissions with ED as first transfer\n",
    "    admissions_ed_S2 = utils.subsetted_by(admissions_ed_S1, ed_first_transfer,\n",
    "                                          [\"subject_id\", \"hadm_id\", \"intime\", \"outtime\"])\n",
    "    transfers_ed_S2 = utils.subsetted_by(transfers_core, admissions_ed_S2, [\"subject_id\", \"hadm_id\"])\n",
    "    admissions_ed_S2.to_csv(SAVE_FD + \"admissions_S2.csv\", index=True, header=True)\n",
    "\n",
    "    \"\"\"\n",
    "    Consider only those admissions for which they did not have a subsequent transfer to a Special ward, which includes\n",
    "    Partum and Psychiatry wards. The full list of wards is identified in WARDS TO REMOVE\n",
    "    \"\"\"\n",
    "    # Remove admissions transferred to irrelevant wards (Partum, Psychiatry). Furthermore, EDObs is also special.\n",
    "    # Missing check that second intime is after ED outtime\n",
    "    transfers_second_ward = utils.compute_second_transfer(transfers_ed_S2, \"subject_id\", \"intime\",\n",
    "                                                          transfers_ed_S2.columns)\n",
    "    transfers_to_relevant_wards = transfers_second_ward[~ transfers_second_ward.careunit.isin(WARDS_TO_REMOVE)]\n",
    "    admissions_ed_S3 = utils.subsetted_by(admissions_ed_S2, transfers_to_relevant_wards, [\"subject_id\", \"hadm_id\"])\n",
    "\n",
    "    # ADD patient core information and next Transfer Information.\n",
    "    patients_S3 = admissions_ed_S3.subject_id.values\n",
    "    admissions_ed_S3.loc[:, PATIENT_INFO] = patients_core.set_index(\"subject_id\").loc[patients_S3, PATIENT_INFO].values\n",
    "\n",
    "    for col in NEXT_TRANSFER_INFO:\n",
    "        admissions_ed_S3.loc[:, \"next_\" + col] = transfers_to_relevant_wards.set_index(\"subject_id\").loc[\n",
    "            patients_S3, col].values\n",
    "\n",
    "    # Compute age and save\n",
    "    admissions_ed_S3[\"age\"] = admissions_ed_S3.intime.dt.year - admissions_ed_S3[\"anchor_year\"] + admissions_ed_S3[\n",
    "        \"anchor_age\"]\n",
    "    admissions_ed_S3.to_csv(SAVE_FD + \"admissions_S3.csv\", index=True, header=True)\n",
    "\n",
    "    \"\"\"\n",
    "    Step 4: Patients must have an age older than AGE LOWERBOUND\n",
    "    \"\"\"\n",
    "    # Compute age and Remove below AGE LOWERBOUND\n",
    "    admissions_ed_S4 = admissions_ed_S3[admissions_ed_S3[\"age\"] >= AGE_LOWERBOUND]\n",
    "    admissions_ed_S4.to_csv(SAVE_FD + \"admissions_S4.csv\", index=True, header=True)\n",
    "\n",
    "    \"\"\"\n",
    "    Step 5: Add ESI information, and subset to patients with ESI values and between 2, 3, 4.\n",
    "    ESI values of 1 and 5 are edge cases (nothing wrong with them, or in extremely critical condition).\n",
    "    \"\"\"\n",
    "    # Compute and remove ESI NAN, ESI 1 and ESI 5 and save\n",
    "    admissions_ed_S4[\"ESI\"] = triage_ed.set_index(\"stay_id\").loc[admissions_ed_S4.stay_id.values, \"acuity\"].values\n",
    "    admissions_ed_S5 = admissions_ed_S4[~ admissions_ed_S4[\"ESI\"].isna()]\n",
    "    # admissions_ed_S5 = admissions_ed_S5[~ admissions_ed_S5[\"ESI\"].isin([1, 5])]\n",
    "\n",
    "    # Save data\n",
    "    admissions_ed_S5.to_csv(SAVE_FD + \"admissions_intermediate.csv\", index=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing script for vitals ED admissions.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Run -admissions_processing.py first.\n",
    "\n",
    "Processing Steps:\n",
    "\n",
    "1. Identify patients computed from admissions_processing.py cohort.\n",
    "2. Consider vitals only between intime and outtime of ED admission.\n",
    "3. Consider only patients with not too much missingness.\n",
    "4. Resample admissions hourly.\n",
    "5. Apply Step 3 to blocked, re-sampled data.\n",
    "\n",
    "\n",
    "Missing Test Functions for Admissions and vitals.\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------ Configuration Params --------------------------------------\n",
    "\"\"\"\n",
    "Global and Argument variables for Vital sign processing.\n",
    "\n",
    "TIME_VARS: list of datetime object columns.\n",
    "ID_COLUMNS: identifiers of patient, admission and hospital stay\n",
    "VITALS_NAMING_DIC: dictionary for renaming columns of dataframe\n",
    "\n",
    "admission_min_count: minimum number of observations per admission\n",
    "vitals_na_threshold: percentage of missing observations deemed \"acceptable\"\n",
    "resampling_rule: frequency of averaged data to consider\n",
    "admission_min_time_to_outtime: minimum length of an admission\n",
    "\"\"\"\n",
    "TIME_VARS = [\"intime\", \"outtime\", \"next_intime\", \"next_outtime\", \"dod\"]\n",
    "ID_COLUMNS = [\"subject_id\", \"hadm_id\", \"stay_id\"]\n",
    "VITALS_NAMING_DIC = {\"temperature\": \"TEMP\", \"heartrate\": \"HR\", \"resprate\": \"RR\",\n",
    "                     \"o2sat\": \"SPO2\", \"sbp\": \"SBP\", \"dbp\": \"DBP\"}\n",
    "\n",
    "admission_min_count = 3\n",
    "vitals_na_threshold = 0.6\n",
    "resampling_rule = \"1H\"\n",
    "admission_min_time_to_outtime = 5\n",
    "\n",
    "\n",
    "def vitals_processing():\n",
    "\n",
    "\t# --------------------- Check admission intermediate previously processed --------------------------------------\n",
    "\t\"\"\"Check admissions processing has been run\"\"\"\n",
    "\n",
    "\ttry:\n",
    "\t\tassert os.path.exists(SAVE_FD + \"admissions_intermediate.csv\")\n",
    "\n",
    "\texcept Exception:\n",
    "\t\tprint(\"Current dir: \", os.getcwd())\n",
    "\t\tprint(\"Path predicted: \", SAVE_FD + \"admissions_intermediate.csv\")\n",
    "\t\traise ValueError(f\"Run admissions_processing.py prior to running '{__file__}'\")\n",
    "\n",
    "\t# ------------------------------------ // --------------------------------------\n",
    "\t\"Load tables\"\n",
    "\n",
    "\n",
    "\t\"\"\"\n",
    "\tLoad tables, and processed admissions.\n",
    "\n",
    "\tadmissions: dataframe indicating the admissions that have been processed.\n",
    "\tvital_signs_ed: dataframe with observation vital sign data in the ED.\n",
    "\t\"\"\"\n",
    "\tadmissions = pd.read_csv(SAVE_FD + \"admissions_intermediate.csv\", index_col=0, header=0, parse_dates=TIME_VARS)\n",
    "\tvital_signs_ed = pd.read_csv(DATA_FD + \"ed/vitalsign.csv\", index_col=0, header=0, low_memory=False,\n",
    "\t\t                   parse_dates=[\"charttime\"])\n",
    "\n",
    "\t# Check correct computation of admissions\n",
    "\ttest.admissions_processed_correctly(admissions)\n",
    "\n",
    "\t# ------------------------------------- // -------------------------------------\n",
    "\t\"\"\"\n",
    "\tProcess Vital Signs. Multiple steps are considered, but vital signs are re-sampled according to resampling rule,\n",
    "\tand then remove based on amount missingness.\n",
    "\t\"\"\"\n",
    "\n",
    "\t\"\"\"\n",
    "\tSubset to admissions pre-processed in admissions_processing.\n",
    "\t\"\"\"\n",
    "\t# Subset to admission sub-cohort and add intime/outtime information\n",
    "\tvitals_S1 = utils.subsetted_by(vital_signs_ed, admissions, \"stay_id\")\n",
    "\tadmissions.set_index(\"stay_id\", inplace=True)\n",
    "\tvitals_S1[[\"intime\", \"outtime\"]] = admissions.loc[vitals_S1.stay_id.values, [\"intime\", \"outtime\"]].values\n",
    "\tvitals_S1.to_csv(SAVE_FD + \"vitals_S1.csv\", index=True, header=True)\n",
    "\n",
    "\t\"\"\"\n",
    "\tSubset observations within intime and outtime of ED admission. Rename columns.\n",
    "\t\"\"\"\n",
    "\t# Subset Endpoints of vital observations according to ED endpoints\n",
    "\tvitals_S2 = vitals_S1[vitals_S1[\"charttime\"].between(vitals_S1[\"intime\"], vitals_S1[\"outtime\"])]\n",
    "\tvitals_S2.rename(VITALS_NAMING_DIC, axis=1, inplace=True)\n",
    "\tvitals_S2.to_csv(SAVE_FD + \"vitals_S2.csv\", index=True, header=True)\n",
    "\n",
    "\t\"\"\"\n",
    "\tRemove admissions with high amounts of missingness.\n",
    "\t\"\"\"\n",
    "\t# Subset to patients with enough data\n",
    "\tvital_feats = list(VITALS_NAMING_DIC.values())\n",
    "\t# vitals_S3 = utils.remove_adms_high_missingness(vitals_S2, vital_feats, \"stay_id\",\n",
    "\t# \t                                     min_count=admission_min_count, min_frac=vitals_na_threshold)\n",
    "\tvitals_S3 = vitals_S2\n",
    "\tvitals_S3.to_csv(SAVE_FD + \"vitals_S3.csv\", index=True, header=True)\n",
    "\n",
    "\t\"\"\"\n",
    "\tCompute time to end of admission, and group observations into blocks.\n",
    "\t\"\"\"\n",
    "\t# Resample admissions according to group length\n",
    "\tvitals_S4 = utils.compute_time_to_end(vitals_S3, id_key=\"stay_id\", time_id=\"charttime\", end_col=\"outtime\")\n",
    "\tvitals_S4 = utils.conversion_to_block(vitals_S4, id_key=\"stay_id\", rule=resampling_rule, time_vars=vital_feats,\n",
    "\t\t                            static_vars=[\"stay_id\", \"intime\", \"outtime\"])\n",
    "\tvitals_S4.to_csv(SAVE_FD + \"vitals_S4.csv\", index=True, header=True)\n",
    "\n",
    "\t\"\"\"\n",
    "\tApply Step 3 again with the blocked data.\n",
    "\t\"\"\"\n",
    "\t# Ensure blocks satisfy conditions - min counts, proportion of missingness AND time to final outcome\n",
    "\tvitals_S5 = utils.remove_adms_high_missingness(vitals_S4, vital_feats, \"stay_id\",\n",
    "\t                                     min_count=admission_min_count, min_frac=vitals_na_threshold)\n",
    "\n",
    "\t\"\"\"\n",
    "\tConsider those admissions with observations with at most an observations 1.5 hours before outtime \n",
    "\t\"\"\"\n",
    "\tvitals_S5 = vitals_S5[vitals_S5[\"time_to_end_min\"].dt.total_seconds() <= admission_min_time_to_outtime * 3600]\n",
    "\tvitals_S5.to_csv(SAVE_FD + \"vitals_intermediate.csv\", index=True, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ecf5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing\n",
    "\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Run -admissions_processing.py and -vitals_processing.py first.\n",
    "\n",
    "Processing Steps:\n",
    "- Subset to admissions identified previously.\n",
    "- Identify windows after admission.\n",
    "- Define targets as one of:\n",
    "a) Death, b) ICU, c) Discharge or d) Ward.\n",
    "\n",
    "\n",
    "Missing Test Functions for Admissions and vitals.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def outcomes_processing():\n",
    "\n",
    "    # ------------------------ Checking Data Loaded -------------------------------\n",
    "    try:\n",
    "        assert os.path.exists(SAVE_FD + \"admissions_intermediate.csv\")\n",
    "        assert os.path.exists(SAVE_FD + \"vitals_intermediate.csv\")\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Run admissions_processing.py and vitals_processing.py prior to running '{__file__}'\")\n",
    "\n",
    "    # ------------------------ Configuration params --------------------------------\n",
    "    \"\"\"\n",
    "    Global and Argument variables for Vital sign processing.\n",
    "    \n",
    "    TIME_VARS: datetime object variables for patient admission.\n",
    "    VITALS_TIME_VARS: datetime object variables for observation data.\n",
    "    \"\"\"\n",
    "\n",
    "    TIME_VARS = [\"intime\", \"outtime\", \"next_intime\", \"next_outtime\", \"dod\"]\n",
    "    VITALS_TIME_VARS = [\"intime\", \"outtime\", \"time_to_end_min\", \"time_to_end_max\",\n",
    "                        \"time_to_end\", f\"sampled_time_to_end({resampling_rule})\"]\n",
    "\n",
    "    # ------------------------ Data Loading ------------------------------\n",
    "\n",
    "    \"\"\"\n",
    "    Load data tables, including pre-processed and raw tables.\n",
    "    \n",
    "    admissions: processed dataframe of ED summary admission data.\n",
    "    vitals: processed dataframe of ED observation data.\n",
    "    transfers_core: dataframe with list of transfers within the hospital system.\n",
    "    \"\"\"\n",
    "    admissions = pd.read_csv(SAVE_FD + \"admissions_intermediate.csv\", index_col=0, header=0, parse_dates=TIME_VARS)\n",
    "    vitals = pd.read_csv(SAVE_FD + \"vitals_intermediate.csv\", index_col=0, header=0, parse_dates=VITALS_TIME_VARS)\n",
    "    transfers_core = pd.read_csv(DATA_FD + \"core/transfers.csv\", index_col=None, header=0,\n",
    "                                 parse_dates=[\"intime\", \"outtime\"])\n",
    "    vitals = utils.convert_to_timedelta(vitals, f\"sampled_time_to_end({resampling_rule})\", \"time_to_end\",\n",
    "                                        \"time_to_end_min\",\n",
    "                                        \"time_to_end_max\")\n",
    "\n",
    "    # Check correct computation of admissions\n",
    "    test.admissions_processed_correctly(admissions)\n",
    "    test.vitals_processed_correctly(vitals)\n",
    "\n",
    "    # ------------------------ Targets Processing -----------------------------\n",
    "\n",
    "    \"\"\"\n",
    "    Process Target Information. Subset transfers and vitals to the relevant set of admissions\n",
    "    \"\"\"\n",
    "\n",
    "    admissions_subset = utils.subsetted_by(admissions, vitals, [\"stay_id\"])\n",
    "    transfers_subset = utils.subsetted_by(transfers_core, admissions_subset, [\"subject_id\", \"hadm_id\"])\n",
    "    vitals[\"chartmax\"] = vitals[\"outtime\"] - vitals[\"time_to_end\"]\n",
    "    vitals[\"hadm_id\"] = admissions.set_index(\"stay_id\").loc[vitals.stay_id.values, \"hadm_id\"].values\n",
    "\n",
    "    \"\"\"\n",
    "    Potential Outcomes will be regular admission to acute ward, ICU or death. We consider 4 different window sizes:\n",
    "    - 4 hours, 12 hours, 24 hours and 48 hours.\n",
    "    \"\"\"\n",
    "    time_window_1 = dt.timedelta(hours=4)\n",
    "    time_window_2 = dt.timedelta(hours=12)\n",
    "    time_window_3 = dt.timedelta(hours=18)\n",
    "    time_window_4 = dt.timedelta(hours=24)\n",
    "    time_window_5 = dt.timedelta(hours=36)\n",
    "    time_window_6 = dt.timedelta(hours=48)\n",
    "\n",
    "    # Need to include Death\n",
    "    outcomes_4_hours = transfers_subset.groupby(\"hadm_id\", as_index=True).progress_apply(\n",
    "        lambda x: utils.select_death_icu_acute(x, admissions_subset, time_window_1))\n",
    "    outcomes_12_hours = transfers_subset.groupby(\"hadm_id\", as_index=True).progress_apply(\n",
    "        lambda x: utils.select_death_icu_acute(x, admissions_subset, time_window_2))\n",
    "    outcomes_18_hours = transfers_subset.groupby(\"hadm_id\", as_index=True).progress_apply(\n",
    "        lambda x: utils.select_death_icu_acute(x, admissions_subset, time_window_3))\n",
    "    outcomes_24_hours = transfers_subset.groupby(\"hadm_id\", as_index=True).progress_apply(\n",
    "        lambda x: utils.select_death_icu_acute(x, admissions_subset, time_window_4))\n",
    "    outcomes_36_hours = transfers_subset.groupby(\"hadm_id\", as_index=True).progress_apply(\n",
    "        lambda x: utils.select_death_icu_acute(x, admissions_subset, time_window_5))\n",
    "    outcomes_48_hours = transfers_subset.groupby(\"hadm_id\", as_index=True).progress_apply(\n",
    "        lambda x: utils.select_death_icu_acute(x, admissions_subset, time_window_6))\n",
    "\n",
    "    # Ensure all patients have only one class\n",
    "    assert outcomes_4_hours.iloc[:, :-1].sum(axis=1).eq(1).all()\n",
    "    assert outcomes_12_hours.iloc[:, :-1].sum(axis=1).eq(1).all()\n",
    "    assert outcomes_18_hours.iloc[:, :-1].sum(axis=1).eq(1).all()\n",
    "    assert outcomes_24_hours.iloc[:, :-1].sum(axis=1).eq(1).all()\n",
    "    assert outcomes_36_hours.iloc[:, :-1].sum(axis=1).eq(1).all()\n",
    "    assert outcomes_48_hours.iloc[:, :-1].sum(axis=1).eq(1).all()\n",
    "\n",
    "    \"\"\"\n",
    "    Final processing, ensure admissions and observations match with patient ids\n",
    "    \"\"\"\n",
    "    # Subset vitals and admission data\n",
    "    admissions_keep = outcomes_4_hours.index.tolist()\n",
    "    admissions_final = admissions_subset[admissions_subset.hadm_id.isin(admissions_keep)]\n",
    "    vitals_final = vitals[vitals.hadm_id.isin(admissions_keep)]\n",
    "\n",
    "    \"\"\"\n",
    "    Add static variables to input data.\n",
    "    \"\"\"\n",
    "    static_vars = [\"gender\", \"age\", \"ESI\"]\n",
    "    vitals_final[static_vars] = admissions_final.set_index(\"hadm_id\").loc[\n",
    "        vitals_final.hadm_id.values, static_vars].values\n",
    "    vitals_final[\"gender\"] = vitals_final.loc[:, \"gender\"].replace(to_replace=[\"M\", \"F\"], value=[1, 0])\n",
    "    vitals_final[\"charttime\"] = vitals_final.loc[:, \"outtime\"].values - vitals_final.loc[:,\n",
    "                                                                        \"sampled_time_to_end(1H)\"].values\n",
    "\n",
    "    \"\"\"\n",
    "    Save Data and Print Basic Information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of Patients and number of observations.\n",
    "    print(f\"Number of cohort patient: {vitals_final.stay_id.nunique()}\")\n",
    "    print(f\"Number of observations: {vitals_final.shape[0]}\")\n",
    "\n",
    "    print(f\"Sample outcome distribution: {outcomes_4_hours.sum(axis=0)}\")\n",
    "\n",
    "    # Save to output variables\n",
    "    process_fd = DATA_FD + \"processed/\"\n",
    "\n",
    "    if not os.path.exists(process_fd):\n",
    "        os.makedirs(process_fd)\n",
    "\n",
    "    # Save general\n",
    "    vitals_final.to_csv(SAVE_FD + \"vitals_final.csv\", index=True, header=True)\n",
    "    admissions_final.to_csv(SAVE_FD + \"admissions_final.csv\", index=True, header=True)\n",
    "\n",
    "    # Save for input\n",
    "    vitals_final.to_csv(process_fd + \"vitals_process.csv\", index=True, header=True)\n",
    "    admissions_final.to_csv(process_fd + \"admissions_process.csv\", index=True, header=True)\n",
    "    outcomes_4_hours.to_csv(process_fd + \"outcomes_4h_process.csv\", index=True, header=True)\n",
    "    outcomes_12_hours.to_csv(process_fd + \"outcomes_12h_process.csv\", index=True, header=True)\n",
    "    outcomes_18_hours.to_csv(process_fd + \"outcomes_18h_process.csv\", index=True, header=True)\n",
    "    outcomes_24_hours.to_csv(process_fd + \"outcomes_24h_process.csv\", index=True, header=True)\n",
    "    outcomes_36_hours.to_csv(process_fd + \"outcomes_36h_process.csv\", index=True, header=True)\n",
    "    outcomes_48_hours.to_csv(process_fd + \"outcomes_48h_process.csv\", index=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72959b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run admissions if not processed\n",
    "    if not os.path.exists(\"data/MIMIC/interim/admissions_intermediate.csv\"):\n",
    "        admissions_processing()\n",
    "\n",
    "    # Run vitals\n",
    "    if not os.path.exists(\"data/MIMIC/interim/vitals_intermediate.csv\"):\n",
    "        vitals_processing()\n",
    "\n",
    "    # Run outcomes\n",
    "    if not os.path.exists(\"data/MIMIC/interim/vitals_final.csv\"):\n",
    "        outcomes_processing()\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984547be",
   "metadata": {},
   "source": [
    "SVM Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Model Class for SVMAll model. SVM is fit to concatenated trajectories.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "SVMALL_INPUT_PARAMS = [\"C\", \"kernel\", \"degree\", \"gamma\", \"coef0\", \"shrinking\", \"probability\", \"tol\", \"class_weight\",\n",
    "                       \"random_state\", \"verbose\"]\n",
    "\n",
    "\n",
    "class SVMAll(SVC):\n",
    "    \"\"\"\n",
    "    Model Class Wrapper for an SVM model training on all (time, feature) pair values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_info: dict = {}, probability: bool = True, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialise object with model configuration.\n",
    "\n",
    "        Params:\n",
    "        - data_info: dict, dictionary containing dataset information, including objects and properties.\n",
    "        - kwargs: model configuration parameters\n",
    "        \"\"\"\n",
    "\n",
    "        # Get proper model_config\n",
    "        self.model_config = {key: value for key, value in kwargs.items() if key in SVMALL_INPUT_PARAMS}\n",
    "\n",
    "        if \"probability\" not in self.model_config.keys():\n",
    "            self.model_config[\"probability\"] = probability\n",
    "\n",
    "        # Initialise other useful information\n",
    "        self.run_num = 1\n",
    "        self.model_name = \"SVMALL\"\n",
    "\n",
    "        # Useful for consistency\n",
    "        self.training_params = {}\n",
    "\n",
    "        # Initialise SVM object with this particular model config\n",
    "        super().__init__(**self.model_config, verbose=True)\n",
    "\n",
    "    def train(self, data_info, **kwargs):\n",
    "        \"\"\"\n",
    "        Wrapper method for fitting the model to input data.\n",
    "\n",
    "        Params:\n",
    "        - probability: bool value, indicating whether model should output hard outcome assignments, or probabilistic.\n",
    "        - data_info: dictionary with data information, objects and parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack relevant data information\n",
    "        X_train, X_val, X_test = data_info[\"X\"]\n",
    "        y_train, y_val, y_test = data_info[\"y\"]\n",
    "        data_name = data_info[\"data_load_config\"][\"data_name\"]\n",
    "\n",
    "        # Update run_num to make space for new experiment\n",
    "        run_num = self.run_num\n",
    "        save_fd = f\"experiments/{data_name}/{self.model_name}/\"\n",
    "\n",
    "        while os.path.exists(save_fd + f\"run{run_num}/\"):\n",
    "            run_num += 1\n",
    "\n",
    "        # make new folder and update run num\n",
    "        os.makedirs(save_fd + f\"run{run_num}/\")\n",
    "        self.run_num = run_num\n",
    "\n",
    "        # Fit to concatenated X_train, X_val\n",
    "        X_train = np.concatenate((X_train, X_val), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "        # Get shape and flatten array\n",
    "        N_test, T, D_f = X_train.shape\n",
    "        X = X_train.reshape(-1, X_train.shape[-1])\n",
    "        y_per_feat = np.repeat(y_train.reshape(-1, 1, 4), repeats=T, axis=1)\n",
    "        y = np.argmax(y_per_feat, axis=-1).reshape(-1)\n",
    "\n",
    "        # Fit model\n",
    "        self.fit(X, y, sample_weight=None)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def analyse(self, data_info, **kwargs):\n",
    "        \"\"\"\n",
    "        Evaluation method to compute and save output results.\n",
    "\n",
    "        Params:\n",
    "        - data_info: dictionary with data information, objects and parameters.\n",
    "\n",
    "        Returns:\n",
    "            - y_pred: dataframe of shape (N, output_dim) with outcome probability prediction.\n",
    "            - outc_pred: Series of shape (N, ) with predicted outcome based on most likely outcome prediction.\n",
    "            - y_true: dataframe of shape (N, output_dim) ith one-hot encoded true outcome.\n",
    "\n",
    "        Saves a variety of model information, as well.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack test data\n",
    "        _, _, X_test = data_info[\"X\"]\n",
    "        _, _, y_test = data_info[\"y\"]\n",
    "\n",
    "        # Get basic data information\n",
    "        data_properties = data_info[\"data_properties\"]\n",
    "        outc_dims = data_properties[\"outc_names\"]\n",
    "        data_load_config = data_info[\"data_load_config\"]\n",
    "        data_name = data_load_config[\"data_name\"]\n",
    "\n",
    "        # Obtain the ids for patients in test set\n",
    "        id_info = data_info[\"ids\"][-1]\n",
    "        pat_ids = id_info[:, 0, 0]\n",
    "\n",
    "        # Define save_fd, track_fd\n",
    "        save_fd = f\"results/{data_name}/{self.model_name}/run{self.run_num}/\"\n",
    "\n",
    "        if not os.path.exists(save_fd):\n",
    "            os.makedirs(save_fd)\n",
    "\n",
    "        # Make prediction on test data\n",
    "        if self.model_config[\"probability\"] is True:\n",
    "            X_test = X_test.reshape(-1, X_test.shape[-1])\n",
    "            output_test = self.predict_proba(X_test).reshape(pat_ids.size, -1, 4)\n",
    "            output_test = np.mean(output_test, axis=1)\n",
    "\n",
    "        else:\n",
    "            # Predict gives categorical vector, and we one-hot encode output.\n",
    "            output_test = np.eye(y_test.shape[-1])[self.predict(X_test)]\n",
    "\n",
    "        # First, compute predicted y estimates\n",
    "        y_pred = pd.DataFrame(output_test, index=pat_ids, columns=outc_dims)\n",
    "        outc_pred = pd.Series(np.argmax(output_test, axis=-1), index=pat_ids)\n",
    "        y_true = pd.DataFrame(y_test, index=pat_ids, columns=outc_dims)\n",
    "\n",
    "\n",
    "        # Define clusters as outcome predicted groups\n",
    "        pis_pred = y_pred\n",
    "        clus_pred = outc_pred\n",
    "\n",
    "        # Get model config\n",
    "        model_config = self.model_config\n",
    "\n",
    "        # ----------------------------- Save Output Data --------------------------------\n",
    "        # Useful objects\n",
    "        y_pred.to_csv(save_fd + \"y_pred.csv\", index=True, header=True)\n",
    "        outc_pred.to_csv(save_fd + \"outc_pred.csv\", index=True, header=True)\n",
    "        clus_pred.to_csv(save_fd + \"clus_pred.csv\", index=True, header=True)\n",
    "        pis_pred.to_csv(save_fd + \"pis_pred.csv\", index=True, header=True)\n",
    "        y_true.to_csv(save_fd + \"y_true.csv\", index=True, header=True)\n",
    "\n",
    "        # save model parameters\n",
    "        with open(save_fd + \"data_config.json\", \"w+\") as f:\n",
    "            json.dump(data_info[\"data_load_config\"], f, indent=4)\n",
    "\n",
    "        with open(save_fd + \"model_config.json\", \"w+\") as f:\n",
    "            json.dump(model_config, f, indent=4)\n",
    "\n",
    "        # Return objects\n",
    "        outputs_dic = {\"save_fd\": save_fd, \"model_config\": self.model_config,\n",
    "                       \"y_pred\": y_pred, \"class_pred\": outc_pred, \"clus_pred\": clus_pred, \"pis_pred\": pis_pred,\n",
    "                       \"y_true\": y_true\n",
    "                       }\n",
    "\n",
    "        # Print Data\n",
    "        print(f\"\\n\\n Results Saved under {save_fd}\")\n",
    "\n",
    "        return outputs_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5b693e",
   "metadata": {},
   "source": [
    "Camelot Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf1b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "<<<<<<< HEAD\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "\"Imports\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import squared_difference, multiply, divide\n",
    "=======\n",
    "Loss, Metrics and Callback functions to use for model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    ">>>>>>> develop\n",
    "import tensorflow.keras.callbacks as cbck\n",
    "\n",
    "from sklearn.metrics import roc_auc_score as roc\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics import adjusted_rand_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import normalized_mutual_info_score, silhouette_score\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "\"Utility Functions and Global Params\"\n",
    "\n",
    "<<<<<<< HEAD\n",
    "=======\n",
    "LOGS_DIR = \"experiments/camelot/\"\n",
    "\n",
    ">>>>>>> develop\n",
    "\n",
    "def log(tensor):\n",
    "    return tf.cast(tf.math.log(tensor + 1e-8), dtype=\"float32\")\n",
    "\n",
    "\n",
    "<<<<<<< HEAD\n",
    "=======\n",
    "def np_log(array):\n",
    "    return np.log(array + 1e-8)\n",
    "\n",
    "\n",
    ">>>>>>> develop\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute confusion matrix\n",
    "    contingency_matrix_ = contingency_matrix(y_true, y_pred)\n",
    "\n",
    "    return np.sum(np.amax(contingency_matrix_, axis=0)) / np.sum(contingency_matrix_)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "\"\"\"Loss Functions\"\"\"\n",
    "\n",
    "\n",
    "def l_pred(y_true, y_pred, weights=None, name='pred_clus_loss'):\n",
    "    \"\"\"\n",
    "<<<<<<< HEAD\n",
    "    Predictive clustering loss\n",
    "\n",
    "    Params:\n",
    "    - y_true: tensor of shape (bs, num_outcs)\n",
    "    - y_pred: tensor of shape (bs, num_outcs)\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for whether weights are given or not\n",
    "    if weights is None:\n",
    "        weights = tf.cast(tf.constant(np.ones(shape=y_true.shape) / y_true.shape[-1]), dtype=np.float32)\n",
    "    else:\n",
    "        weights = tf.convert_to_tensor(value=weights, dtype=\"float32\")\n",
    "\n",
    "    # Compute batch and return\n",
    "    batch_loss = - tf.reduce_mean(tf.reduce_sum(weights * y_true * log(y_pred), axis=-1), name=name)\n",
    "\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "def l_clus(clusters, name='emb_sep_L'):\n",
    "    \"\"\"\n",
    "    Cluster representation separation loss.\n",
    "\n",
    "    Params:\n",
    "    - clusters: tensor of shape (K, _ )\n",
    "    \"\"\"\n",
    "\n",
    "    # Expand dims to take advantage of broadcasting\n",
    "    embedding_column = tf.expand_dims(clusters, axis=1)          # shape (K, 1, _)\n",
    "    embedding_row = tf.expand_dims(clusters, axis=0)             # shape (1, K, _)\n",
    "\n",
    "    # Compute L1 distance\n",
    "    pairwise_loss = - tf.reduce_sum((embedding_column - embedding_row) ** 2, axis=-1)  # shape K, K\n",
    "    loss = tf.reduce_sum(pairwise_loss, axis=None, name=name)\n",
    "\n",
    "    # normalise by K(K-1=/2\n",
    "    K = clusters.get_shape()[0]\n",
    "    norm_factor = K * (K - 1) / 2\n",
    "    norm_loss = tf.math.divide(loss, tf.cast(norm_factor, dtype=\"float32\"))\n",
    "=======\n",
    "    Negative weighted predictive clustering loss. Computes Cross-entropy between categorical y_true and y_pred.\n",
    "    This is minimised when y_pred matches y_true.\n",
    "\n",
    "    Params:\n",
    "    - y_true: array-like of shape (bs, num_outcs) of one-hot encoded true class.\n",
    "    - y_pred: array-like of shape (bs, num_outcs) of probability class predictions.\n",
    "    - weights: array-like of shape (num_outcs) of weights to multiply cross-entropy terms. (default None).\n",
    "    - name: name to give to operation.\n",
    "\n",
    "    If weights is None, defaults to regular cross-entropy calculation.\n",
    "\n",
    "    Returns:\n",
    "    - loss_value: score indicating corresponding loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # If weights is None, return weights as set of 1s.\n",
    "    if weights is None:\n",
    "        weights = tf.cast(tf.constant(np.ones(shape=y_true.shape) / y_true.shape[-1]), dtype=np.float32)\n",
    "\n",
    "    # Compute batch\n",
    "    batch_neg_ce = tf.reduce_sum(weights * y_true * log(y_pred), axis=-1)\n",
    "\n",
    "    # Average over batch\n",
    "    loss_value = tf.reduce_mean(batch_neg_ce, name=name)\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "def l_clus(cluster_reps, name='embedding_sep_loss'):\n",
    "    \"\"\"Compute Embedding separation Loss on embedding vectors.\"\"\"\n",
    "    \"\"\"Cluster representation separation loss. Computes negative euclidean distance summed over pairs of cluster \n",
    "    representation vectors. This loss is minimised as cluster vectors are separated \n",
    "\n",
    "    Params:\n",
    "    - cluster_reps: array-like of shape (K, latent_dim) of cluster representation vectors.\n",
    "    - name: name to give to operation.\n",
    "\n",
    "    Returns:\n",
    "    - norm_loss: score indicating corresponding loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Expand input to allow broadcasting\n",
    "    embedding_column = tf.expand_dims(cluster_reps, axis=1)  # shape (K, 1, latent_dim)\n",
    "    embedding_row = tf.expand_dims(cluster_reps, axis=0)  # shape (1, K, latent_dim)\n",
    "\n",
    "    # Compute pairwise Euclidean distance between cluster vectors, and sum over pairs of clusters.\n",
    "    pairwise_loss = tf.reduce_sum((embedding_column - embedding_row) ** 2, axis=-1)\n",
    "    loss = - tf.reduce_sum(pairwise_loss, axis=None, name=name)\n",
    "\n",
    "    # normalise by factor\n",
    "    K = cluster_reps.get_shape()[0]\n",
    "    norm_loss = loss / (K * (K - 1))\n",
    ">>>>>>> develop\n",
    "\n",
    "    return norm_loss\n",
    "\n",
    "\n",
    "<<<<<<< HEAD\n",
    "def l_dist(clus_prob, name = \"clus_sel_loss\"):\n",
    "    \"\"\"\n",
    "    Cluster selection loss.\n",
    "\n",
    "    Params:\n",
    "    - clus_prob: tensor of shape (batch_size, num_clusters).\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute average distribution over each cluster\n",
    "    avg_prob_per_clus = tf.reduce_mean(clus_prob, axis=-1, name=name)\n",
    "\n",
    "    # Compute negative entropy - minimised for uniform distribution.\n",
    "    neg_entropy = tf.reduce_sum(multiply(avg_prob_per_clus, log(avg_prob_per_clus)))\n",
    "=======\n",
    "def l_dist(clusters_prob):\n",
    "    \"\"\"\n",
    "    Cluster distribution loss. Computes negative entropy of cluster distribution probability values.\n",
    "    This is minimised when the cluster distribution is uniform (all clusters similar size).\n",
    "\n",
    "    Params:\n",
    "    - clusters_prob: array-like of shape (bs, K) of cluster_assignments distributions.\n",
    "    - name: name to give to operation.\n",
    "\n",
    "    Returns:\n",
    "    - loss_value: score indicating corresponding loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate average cluster assignment distribution\n",
    "    clus_avg_prob = tf.reduce_mean(clusters_prob, axis=0)\n",
    "\n",
    "    # Compute negative entropy\n",
    "    neg_entropy = tf.reduce_sum(clus_avg_prob * log(clus_avg_prob))\n",
    ">>>>>>> develop\n",
    "\n",
    "    return neg_entropy\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "<<<<<<< HEAD\n",
    "\"Useful information to print during training.\"\n",
    "\n",
    "\n",
    "class y_clus_cross_entropy(cbck.Callback):\n",
    "    \"\"\"\n",
    "    Compute normalised Cross-Entropy Loss between cluster phenotypes.\n",
    "    Smaller values represent more separation of y_clusters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, validation_data: tuple = (), interval: int = 5):\n",
    "        super().__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, _ = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            ce_sep, epsilon = 0, 1e-9\n",
    "            K = self.model.embeddings.numpy().shape[0]\n",
    "\n",
    "            # Compute embedding phenotypes\n",
    "            clus_phenotypes = self.model.Predictor(self.model.embeddings).numpy() + epsilon\n",
    "=======\n",
    "\"Callback methods to update training procedure.\"\n",
    "\n",
    "\n",
    "class CEClusSeparation(cbck.Callback):\n",
    "    \"\"\"\n",
    "    Callback method to print Normalised Cross-Entropy separation between cluster phenotypes.\n",
    "    Higher values indicate higher separation (which is good!)\n",
    "\n",
    "    Params:\n",
    "    - validation_data: tuple of X_val, y_val data\n",
    "    - weighted: bool, indicating whether outcomes should be weighted. (default = False)\n",
    "    - interval: interval between epochs on which to print values. (default = 5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, validation_data: tuple, weighted: bool = False, interval: int = 5):\n",
    "        super().__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "        # Compute outcome weights if weighted=True\n",
    "        if weighted is True:\n",
    "            class_numbers = tf.reduce_sum(self.y_val, axis=0).numpy()\n",
    "            self.weights = class_numbers / np.sum(class_numbers)\n",
    "\n",
    "        else:\n",
    "            self.weights = np.ones(shape=(self.y_val.get_shape()[0]))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None, **kwargs):\n",
    "\n",
    "        # Print information if matches interval epoch length\n",
    "        if epoch % self.interval == 0:\n",
    "\n",
    "            # Initialise callback value, and determine K\n",
    "            cbck_value, K = 0, self.model.cluster_reps.numpy().shape[0]\n",
    "            clus_phenotypes = self.model.Predictor(self.model.cluster_reps).numpy()\n",
    ">>>>>>> develop\n",
    "\n",
    "            # Iterate over all pairs of clusters and compute symmetric CE\n",
    "            for i in range(K):\n",
    "                for j in range(i + 1, K):\n",
    "<<<<<<< HEAD\n",
    "                    ce_sep += - np.sum(clus_phenotypes[i, :] * np.log(clus_phenotypes[j, :]))\n",
    "                    ce_sep += - np.sum(clus_phenotypes[j, :] * np.log(clus_phenotypes[i, :]))\n",
    "\n",
    "            # normalise and print output\n",
    "            norm_loss = ce_sep / (K * (K + 1))\n",
    "            print(\"End of Epoch {:d} - CE sep : {:.4f}\".format(epoch, norm_loss))\n",
    "\n",
    "\n",
    "class ConfusionMatrix(cbck.Callback):\n",
    "    \"\"\"Display Confusion Matrix of predicted outcomes vs target outcomes.\"\"\"\n",
    "\n",
    "    def __init__(self, validation_data=(), interval=5):\n",
    "        super().__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.C = self.y_val.shape[-1]  # num_classes\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            cm_output = np.zeros(shape=(self.C, self.C))\n",
    "\n",
    "            # Compute prediction and true values in categorical format.\n",
    "            model_output = (self.model(self.X_val)).numpy()\n",
    "            y_pred = np.argmax(model_output, axis=-1)\n",
    "            y_true = np.argmax(self.y_val, axis=-1)\n",
    "\n",
    "            # Iterate through classes\n",
    "            for true_class in range(self.C):\n",
    "                for pred_class in range(self.C):\n",
    "                    num_samples = np.logical_and(y_pred == pred_class, y_true == true_class).sum()\n",
    "                    cm_output[true_class, pred_class] = num_samples\n",
    "\n",
    "            print(\"End of Epoch {:d} - Confusion matrix: \\n {}\".format(epoch, cm_output.astype(int)))\n",
    "\n",
    "\n",
    "class AUROC(cbck.Callback):\n",
    "    \"\"\"Compute AUROC on Validation Data.\"\"\"\n",
    "\n",
    "    def __init__(self, validation_data=(), interval=5):\n",
    "=======\n",
    "                    cbck_value += - np.sum(clus_phenotypes[i, :] * np_log(clus_phenotypes[j, :]))\n",
    "                    cbck_value += - np.sum(clus_phenotypes[j, :] * np_log(clus_phenotypes[i, :]))\n",
    "\n",
    "            # normalise and print output\n",
    "            cbck_value = cbck_value / (K * (K + 1))\n",
    "\n",
    "            print(\"End of Epoch {:d} - CE sep : {:.4f}\".format(epoch, cbck_value))\n",
    "\n",
    "\n",
    "class ConfusionMatrix(cbck.Callback):\n",
    "    \"\"\"\n",
    "    Callback method to print Confusion Matrix over data.\n",
    "\n",
    "    Output is a matrix indicating the amount of patients assigned to a target class and with a certain true class.\n",
    "\n",
    "    Params:\n",
    "    - validation_data: tuple of X_val, y_val data\n",
    "    - interval: interval between epochs on which to print values. (default = 5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, validation_data: tuple, interval: int = 5):\n",
    "        super().__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "        # Compute number of outcomes\n",
    "        self.num_outcs = self.y_val.shape[-1]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        # Print information if matches interval epoch length\n",
    "        if epoch % self.interval == 0:\n",
    "\n",
    "            # Initialise output Confusion matrix\n",
    "            cm_output = np.zeros(shape=(self.num_outcs, self.num_outcs))\n",
    "\n",
    "            # Compute prediction and true values in categorical format.\n",
    "            y_pred = (self.model(self.X_val)).numpy()\n",
    "            class_pred = np.argmax(y_pred, axis=-1)\n",
    "            class_true = np.argmax(self.y_val, axis=-1)\n",
    "\n",
    "            # Iterate through classes\n",
    "            for true_class in range(self.num_outcs):\n",
    "                for pred_class in range(self.num_outcs):\n",
    "                    num_samples = np.logical_and(class_pred == pred_class, class_true == true_class).sum()\n",
    "                    cm_output[true_class, pred_class] = num_samples\n",
    "\n",
    "            # Print as pd.dataframe\n",
    "            index = [f\"TC{class_}\" for class_ in range(1, self.num_outcs + 1)]\n",
    "            columns = index\n",
    "\n",
    "            cm = pd.DataFrame(cm_output, index=index, columns=columns)\n",
    "\n",
    "            print(\"End of Epoch {:d} - Confusion matrix: \\n {}\".format(epoch, cm.astype(int)))\n",
    "\n",
    "\n",
    "class AUROC(cbck.Callback):\n",
    "    \"\"\"\n",
    "    Callback method to display AUROC value for predicted y.\n",
    "\n",
    "    Params:\n",
    "    - validation_data: tuple of X_val, y_val data\n",
    "    - interval: interval between epochs on which to print values. (default = 5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, validation_data: tuple, interval: int = 5):\n",
    ">>>>>>> develop\n",
    "        super().__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "<<<<<<< HEAD\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "=======\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    ">>>>>>> develop\n",
    "        if epoch % self.interval == 0:\n",
    "            # Compute predictions\n",
    "            y_pred = self.model(self.X_val).numpy()\n",
    "\n",
    "            # Compute ROC\n",
    "            roc_auc_score = roc(y_true=self.y_val, y_score=y_pred,\n",
    "                                average=None, multi_class='ovr')\n",
    "\n",
    "<<<<<<< HEAD\n",
    "            print(\"End of Epoch {:d} - ROC score: {}\".format(epoch, roc_auc_score))\n",
    "\n",
    "\n",
    "class PrintClustersUsed(cbck.Callback):\n",
    "    \"\"\"Print Number of Clusters and Cluster Distribution with samples assigned to them.\"\"\"\n",
    "\n",
    "    def __init__(self, validation_data=(), interval=5):\n",
    "=======\n",
    "            print(\"End of Epoch {:d} - OVR ROC score: {}\".format(epoch, roc_auc_score))\n",
    "\n",
    "\n",
    "class PrintClusterInfo(cbck.Callback):\n",
    "    \"\"\"\n",
    "    Callback method to display cluster distribution information assignment.\n",
    "\n",
    "    Params:\n",
    "    - validation_data: tuple of X_val, y_val data\n",
    "    - interval: interval between epochs on which to print values. (default = 5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, validation_data: tuple, interval: int = 5):\n",
    ">>>>>>> develop\n",
    "        super().__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "<<<<<<< HEAD\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            # Compute cluster assignment\n",
    "            clus_pred = self.model.Identifier(self.model.Encoder(self.X_val)).numpy()\n",
    "            num_clusters = np.unique(np.argmax(clus_pred, axis=-1))\n",
    "            avg_cluster_dist = np.mean(clus_pred, axis=-1)\n",
    "\n",
    "            # Print Information\n",
    "            print(\"End of Epoch {:d} - num_clusters {} - cluster dist {}\".format(epoch, num_clusters, avg_cluster_dist))\n",
    "\n",
    "\n",
    "class SupervisedTargetMetrics(cbck.Callback):\n",
    "    \"\"\"Print Scores for all given Supervised Target Metrics (NMI, ARS, Purity) on validation data during training.\"\"\"\n",
    "\n",
    "    def __init__(self, validation_data=(), interval=5):\n",
    "=======\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.interval == 0:\n",
    "\n",
    "            # Compute cluster_predictions\n",
    "            clus_pred = self.model.Identifier(self.model.Encoder(self.X_val)).numpy()\n",
    "            clus_assign = np.argmax(clus_pred, axis=-1)\n",
    "            K = clus_pred.shape[-1]\n",
    "\n",
    "            # Compute \"hard\" cluster assignment numbers\n",
    "            hard_cluster_num = np.zeros(shape=K)\n",
    "            for clus_id in range(self.K):\n",
    "                hard_cluster_num[clus_id] = np.sum(clus_assign == clus_id)\n",
    "            hard_cluster_num = pd.Series(hard_cluster_num, index=[f\"Clus {k}\" for k in range(1, K + 1)])\n",
    "\n",
    "            # Compute average cluster distribution\n",
    "            avg_cluster_dist = np.mean(clus_pred, axis=0)\n",
    "\n",
    "            # Print Information\n",
    "            print(\"End of Epoch {:d} - hard_cluster_info {} - avg cluster prob dist {}\".format(epoch, hard_cluster_num,\n",
    "                                                                                               avg_cluster_dist))\n",
    "\n",
    "\n",
    "class SupervisedTargetMetrics(cbck.Callback):\n",
    "    \"\"\"\n",
    "    Callback method to display supervised target metrics: Normalised Mutual Information, Adjusted Rand Score and\n",
    "    Purity Score\n",
    "\n",
    "    Params:\n",
    "    - validation_data: tuple of X_val, y_val data\n",
    "    - interval: interval between epochs on which to print values. (default = 5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, validation_data: tuple, interval: int = 5):\n",
    ">>>>>>> develop\n",
    "        super().__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "<<<<<<< HEAD\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            # Compute y_pred, y_true in categorical format.\n",
    "            model_output = (self.model(self.X_val)).numpy()\n",
    "            y_pred = np.argmax(model_output, axis=-1)\n",
    "            y_true = np.argmax(self.y_val, axis=-1).reshape(-1)\n",
    "\n",
    "            # Target metrics\n",
    "            nmi = normalized_mutual_info_score(labels_true=y_true, labels_pred=y_pred)\n",
    "            ars = adjusted_rand_score(labels_true=y_true, labels_pred=y_pred)\n",
    "            purity = purity_score(y_true=y_true, y_pred=y_pred)\n",
    "=======\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.interval == 0:\n",
    "            # Compute y_pred, y_true in categorical format.\n",
    "            model_output = (self.model(self.X_val)).numpy()\n",
    "            class_pred = np.argmax(model_output, axis=-1)\n",
    "            class_true = np.argmax(self.y_val, axis=-1).reshape(-1)\n",
    "\n",
    "            # Target metrics\n",
    "            nmi = normalized_mutual_info_score(labels_true=class_true, labels_pred=class_pred)\n",
    "            ars = adjusted_rand_score(labels_true=class_true, labels_pred=class_pred)\n",
    "            purity = purity_score(y_true=class_true, y_pred=class_pred)\n",
    ">>>>>>> develop\n",
    "\n",
    "            print(\"End of Epoch {:d} - NMI {:.2f} , ARS {:.2f} , Purity {:.2f}\".format(epoch, nmi, ars, purity))\n",
    "\n",
    "\n",
    "class UnsupervisedTargetMetrics(cbck.Callback):\n",
    "<<<<<<< HEAD\n",
    "    \"\"\"Print Scores for all Unsupervised metrics (DBS, CHS, SIL) on validation data (inc. latent) over training.\"\"\"\n",
    "\n",
    "    def __init__(self, validation_data=(), interval=5):\n",
    "        super().__init__()\n",
    "        self.interval = interval\n",
    "        self.latent_reps = None\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            # Compute predictions and latent representations\n",
    "            self.latent_reps = self.model.Encoder(self.X_val)\n",
    "            model_output = (self.model(self.X_val)).numpy()\n",
    "            y_pred = np.argmax(model_output, axis=-1)\n",
    "=======\n",
    "    \"\"\"\n",
    "    Callback method to display unsupervised target metrics: Davies-Bouldin Score, Calinski-Harabasz Score,\n",
    "    Silhouette Score\n",
    "\n",
    "    Params:\n",
    "    - validation_data: tuple of X_val, y_val data\n",
    "    - interval: interval between epochs on which to print values. (default = 5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, validation_data: tuple, interval: int = 5):\n",
    "        super().__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.interval == 0:\n",
    "            # Compute predictions and latent representations\n",
    "            latent_reps = self.model.Encoder(self.X_val)\n",
    "            model_output = (self.model(self.X_val)).numpy()\n",
    "            clus_pred = np.argmax(model_output, axis=-1)\n",
    ">>>>>>> develop\n",
    "\n",
    "            # Reshape input data and allow feature comparison\n",
    "            X_val_2d = np.reshape(self.X_val, (self.X_val[0], -1))\n",
    "\n",
    "            # Compute metrics\n",
    "<<<<<<< HEAD\n",
    "            dbs = davies_bouldin_score(X_val_2d, labels=y_pred)\n",
    "            dbs_l = davies_bouldin_score(self.latent_reps, labels=y_pred)\n",
    "            chs = calinski_harabasz_score(X_val_2d, labels=y_pred)\n",
    "            chs_l = calinski_harabasz_score(self.latent_reps, labels=y_pred)\n",
    "            sil = silhouette_score(X=X_val_2d, labels=y_pred, random_state=self.model.seed)\n",
    "            sil_l = silhouette_score(X=self.latent_reps, labels=y_pred, random_state=self.model.seed)\n",
    "\n",
    "            print(f\"\"\"End of Epoch {epoch:d} (score, score on latent): \n",
    "=======\n",
    "            dbs = davies_bouldin_score(X_val_2d, labels=clus_pred)\n",
    "            dbs_l = davies_bouldin_score(latent_reps, labels=clus_pred)\n",
    "            chs = calinski_harabasz_score(X_val_2d, labels=clus_pred)\n",
    "            chs_l = calinski_harabasz_score(latent_reps, labels=clus_pred)\n",
    "            sil = silhouette_score(X=X_val_2d, labels=clus_pred, random_state=self.model.seed)\n",
    "            sil_l = silhouette_score(X=latent_reps, labels=clus_pred, random_state=self.model.seed)\n",
    "\n",
    "            print(f\"\"\"End of Epoch {epoch:d} (score, latent score): \n",
    ">>>>>>> develop\n",
    "                        DBS {dbs:.2f}, {dbs_l:.2f}   \n",
    "                        CHS {chs:.2f}, {chs_l:.2f}  \n",
    "                        SIL {sil:.2f}, {sil_l:.2f}\"\"\")\n",
    "\n",
    "\n",
    "<<<<<<< HEAD\n",
    "def compute_metric(metric_name):\n",
    "    \"\"\"Given metric shorthand, return corresponding callback.\"\"\"\n",
    "    if \"auc\" == metric_name.lower() or \"roc\" == metric_name.lower():\n",
    "        return AUROC\n",
    "\n",
    "    elif \"ce_sep\" == metric_name.lower():\n",
    "        return CESeparation\n",
    "\n",
    "    elif \"conf_matrix\" == metric_name.lower():\n",
    "        return ConfusionMatrix\n",
    "\n",
    "    elif \"clus_dist\" == metric_name.lower():\n",
    "        return PrintClustersUsed\n",
    "\n",
    "    elif \"sup_targets\" == metric_name.lower():\n",
    "        return SupervisedTargetMetrics\n",
    "\n",
    "    elif \"unsup_targets\" == metric_name.lower():\n",
    "        return UnsupervisedTargetMetrics\n",
    "\n",
    "\n",
    "def get_callbacks(track_loss, early_stop=True, lr_scheduler=True, tensorboard=True, min_delta=0.0001, patience=100):\n",
    "    \"\"\"Generate list of callbacks, given input params.\n",
    "\n",
    "    Params:\n",
    "        - track_loss: str, name of main loss to keep track of.\n",
    "=======\n",
    "def cbck_list(summary_name: str):\n",
    "    \"\"\"\n",
    "    Shorthand for callbacks above.\n",
    "\n",
    "    Params:\n",
    "    - summary_name: str containing shorthands for different callbacks.\n",
    "    \"\"\"\n",
    "    extra_callback_list = set([])\n",
    "\n",
    "    if \"auc\" in summary_name.lower() or \"roc\" in summary_name.lower():\n",
    "        extra_callback_list.update([AUROC])\n",
    "\n",
    "    if \"clus_sep\" in summary_name.lower() or \"clus_phen\" in summary_name.lower():\n",
    "        extra_callback_list.update([CEClusSeparation])\n",
    "\n",
    "    if \"cm\" in summary_name.lower() or \"conf_matrix\" in summary_name.lower():\n",
    "        extra_callback_list.update([ConfusionMatrix])\n",
    "\n",
    "    if \"clus_info\" in summary_name.lower():\n",
    "        extra_callback_list.update([PrintClusterInfo])\n",
    "\n",
    "    if \"sup_scores\" in summary_name.lower():\n",
    "        extra_callback_list.update([SupervisedTargetMetrics])\n",
    "\n",
    "    if \"unsup_scores\" in summary_name.lower():\n",
    "        extra_callback_list.update([UnsupervisedTargetMetrics])\n",
    "\n",
    "    return list(extra_callback_list)\n",
    "\n",
    "\n",
    "def get_callbacks(track_loss: str, other_cbcks: str = \"\", early_stop: bool = True, lr_scheduler: bool = True,\n",
    "                  tensorboard: bool = True,\n",
    "                  min_delta: float = 0.0001, patience: int = 100):\n",
    "    \"\"\"\n",
    "    Generate complete list of callbacks given input configuration.\n",
    "\n",
    "    Params:\n",
    "        - track_loss: str, name of main loss to keep track of.\n",
    "        - other_cbcks: str, list of other callbacks to consider (default = \"\", which selects None).\n",
    ">>>>>>> develop\n",
    "        - early_stop: whether to stop training early in case of no progress. (default = True)\n",
    "        - lr_scheduler: dynamically update learning rate. (default = True)\n",
    "        - tensorboard: write tensorboard friendly logs which can then be visualised. (default = True)\n",
    "        - min_delta: if early stopping, the interval on which to check improvement or not.\n",
    "        - patience: how many epochs to wait until checking for improvements.\n",
    "        \"\"\"\n",
    "<<<<<<< HEAD\n",
    "    cbck_list = []\n",
    "\n",
    "    # Handle saving paths and folders\n",
    "    logs_dir = \"experiments/main/\"\n",
    "=======\n",
    "    callbacks = set([])\n",
    "\n",
    "    # Handle saving paths and folders\n",
    "    logs_dir = LOGS_DIR\n",
    ">>>>>>> develop\n",
    "    if not os.path.exists(logs_dir):\n",
    "        os.makedirs(logs_dir)\n",
    "\n",
    "    # Save Folder is first run that has not been previously computed\n",
    "    run_num = 1\n",
    "    while os.path.exists(logs_dir + f\"run{run_num}/\"):\n",
    "        run_num += 1\n",
    "<<<<<<< HEAD\n",
    "    save_fd = logs_dir + f\"run{run_num}/\"\n",
    "    assert not os.path.exists(save_fd)\n",
    "    os.makedirs(save_fd)\n",
    "    os.makedirs(save_fd + \"logs/\")\n",
    "\n",
    "    # Model Weight saving callback\n",
    "    checkpoint = cbck.ModelCheckpoint(filepath=save_fd + \"models/checkpoints/epoch-{epoch}\", save_best_only=True,\n",
    "                                      monitor=track_loss, save_freq=\"epoch\")\n",
    "    csv_logger = cbck.CSVLogger(filename=save_fd + \"loss_tracker.csv\", separator=\",\", append=True)\n",
    "    cbck_list.append(checkpoint)\n",
    "    cbck_list.append(csv_logger)\n",
    "\n",
    "    if early_stop:\n",
    "        cbck_list.append(cbck.EarlyStopping(monitor='val_' + track_loss, mode=\"min\", restore_best_weights=True,\n",
    "                                            min_delta=min_delta, patience=patience))\n",
    "\n",
    "    if lr_scheduler:\n",
    "        cbck_list.append(cbck.ReduceLROnPlateau(monitor='val_' + track_loss, mode='min', cooldown=15,\n",
    "                                                min_lr=0.00001, factor=0.25))\n",
    "\n",
    "    if tensorboard:\n",
    "        cbck_list.append(cbck.TensorBoard(log_dir=save_fd + \"logs/\", histogram_freq=1))\n",
    "\n",
    "    return cbck_list, run_num\n",
    "=======\n",
    "\n",
    "    # Save as new run\n",
    "    save_fd = logs_dir + f\"run{run_num}/\"\n",
    "    assert not os.path.exists(save_fd)\n",
    "\n",
    "    os.makedirs(save_fd)\n",
    "    os.makedirs(save_fd + \"logs/\")\n",
    "\n",
    "    # ------------------ Start Loading callbacks ---------------------------\n",
    "\n",
    "    # Load custom callbacks first\n",
    "    callbacks.update(cbck_list(other_cbcks))\n",
    "\n",
    "    # Model Weight saving callback\n",
    "    checkpoint = cbck.ModelCheckpoint(filepath=save_fd + \"models/checkpoints/epoch-{epoch}\", save_best_only=True,\n",
    "                                      monitor=track_loss, save_freq=\"epoch\")\n",
    "    callbacks.update([checkpoint])\n",
    "\n",
    "    # Logging Loss values\n",
    "    csv_logger = cbck.CSVLogger(filename=save_fd + \"training/loss_tracker.csv\", separator=\",\", append=True)\n",
    "    callbacks.update([csv_logger])\n",
    "\n",
    "    # Check if Early stoppage is added\n",
    "    if early_stop:\n",
    "        callbacks.update([cbck.EarlyStopping(monitor='val_' + track_loss, mode=\"min\", restore_best_weights=True,\n",
    "                                             min_delta=min_delta, patience=patience)])\n",
    "\n",
    "    # Check if LR Scheduling is in place\n",
    "    if lr_scheduler:\n",
    "        callbacks.update([cbck.ReduceLROnPlateau(monitor='val_' + track_loss, mode='min', cooldown=15,\n",
    "                                                 min_lr=0.00001, factor=0.25)])\n",
    "\n",
    "    # Check if Tensorboard is active\n",
    "    if tensorboard:\n",
    "        callbacks.update([cbck.TensorBoard(log_dir=save_fd + \"logs/\", histogram_freq=1)])\n",
    "\n",
    "    return callbacks, run_num\n",
    ">>>>>>> develop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------ Import Libraries ---------------\n",
    "import tensorflow as tf\n",
    "from tensorflow import linalg\n",
    "from tensorflow.keras.layers import Dense, Dropout, Layer, LSTM\n",
    "from tensorflow.keras.regularizers import l1_l2 as mix_l1_l2_reg\n",
    "\n",
    "\n",
    "# ------------ Utility Functions --------------\n",
    "<<<<<<< HEAD\n",
    "def _norm_abs(inputs, axis=1):\n",
    "    \"\"\"\n",
    "    Given input data, compute normalisation according to L1 metric for given axis 1.\n",
    "    \"\"\"\n",
    "    abs_input = tf.math.abs(inputs)  # Compute vector of absolute values.\n",
    "\n",
    "    return tf.math.divide(abs_input, tf.reduce_sum(abs_input, axis=axis, keepdims=True))\n",
    "\n",
    "\n",
    "def _estimate_alpha(feature_reps, targets):\n",
    "    \"\"\"\n",
    "    alpha parameters OLS estimation given projected input features and targets.\n",
    "    feature_reps: shape (bs, T, d, units)\n",
    "    targets: shape (bs, T, units)\n",
    "=======\n",
    "def _estimate_alpha(feature_reps, targets):\n",
    "    \"\"\"\n",
    "    alpha parameters OLS estimation given projected input features and targets.\n",
    "\n",
    "    Params:\n",
    "    - feature_reps: array-like of shape (bs, T, d, units)\n",
    "    - targets: array-like of shape (bs, T, units)\n",
    "\n",
    "    returns:\n",
    "    - un-normalised alpha weights: array-like of shape (bs, T, d)\n",
    ">>>>>>> develop\n",
    "    \"\"\"\n",
    "    X_T, X = feature_reps, linalg.matrix_transpose(feature_reps)\n",
    "\n",
    "    # Compute matrix inversion\n",
    "    X_TX_inv = linalg.inv(linalg.matmul(X_T, X))\n",
    "    X_Ty = linalg.matmul(X_T, tf.expand_dims(targets, axis=-1))\n",
    "\n",
    "    # Compute likely scores\n",
    "    alpha_hat = linalg.matmul(X_TX_inv, X_Ty)\n",
    "\n",
    "<<<<<<< HEAD\n",
    "    return tf.squeeze(alpha_hat)  # shape (bs, T, d) (NOT normalised)\n",
    "=======\n",
    "    return tf.squeeze(alpha_hat)\n",
    ">>>>>>> develop\n",
    "\n",
    "\n",
    "def _estimate_gamma(o_hat, cluster_targets):\n",
    "    \"\"\"\n",
    "    Estimate gamma parameters through OLS estimation given projected input features and targets.\n",
    "<<<<<<< HEAD\n",
    "    o_hat: shape (bs, T, units)\n",
    "    targets: shape (K, units)\n",
    "=======\n",
    "\n",
    "    Params:\n",
    "    - o_hat: array-like of shape (bs, T, units)\n",
    "    - targets: array-like of shape (K, units)\n",
    "\n",
    "    returns:\n",
    "    - gamma_weights: array-like of shape (bs, K, T)\n",
    ">>>>>>> develop\n",
    "    \"\"\"\n",
    "    X_T = tf.expand_dims(o_hat, axis=1)\n",
    "    X = linalg.matrix_transpose(X_T)\n",
    "    y = tf.expand_dims(tf.expand_dims(cluster_targets, axis=0), axis=-1)\n",
    "\n",
    "    # Compute inversion\n",
    "    X_TX_inv = linalg.inv(linalg.matmul(X_T, X))\n",
    "    X_Ty = linalg.matmul(X_T, y)\n",
    "\n",
    "    # Compute gamma\n",
    "    gamma_hat = linalg.matmul(X_TX_inv, X_Ty)\n",
    "\n",
    "    return tf.squeeze(gamma_hat)\n",
    "\n",
    "\n",
    "<<<<<<< HEAD\n",
    "=======\n",
    "def _norm_abs(array, axis: int = 1):\n",
    "    \"\"\"\n",
    "    Compute L1 normalisation of array according to axis.\n",
    "\n",
    "    Params:\n",
    "    - array: array-like object.\n",
    "    - axis: integer.\n",
    "\n",
    "    returns:\n",
    "    - normalised array according to axis.\n",
    "    \"\"\"\n",
    "    array_abs = tf.math.abs(array)\n",
    "\n",
    "    output = array_abs / tf.reduce_sum(array_abs, axis=axis, keepdims=True)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    ">>>>>>> develop\n",
    "# ------------ MLP definition ---------------\n",
    "class MLP(Layer):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron (MLP) neural network architecture.\n",
    "\n",
    "    Params:\n",
    "    - output_dim : int, dimensionality of output space for each sub-sequence.\n",
    "    - hidden_layers : int, Number of \"hidden\" feedforward layers. (default = 2)\n",
    "<<<<<<< HEAD\n",
    "    - hidden_nodes : int, For \"hidden\" feedforward layers, the dimensionality  of the output space. (default = 30)\n",
    "    - activation_fn : str/fn, The activation function to use. (default = 'sigmoid')\n",
    "    - output_fn : str/fn, The activation function on the output of the MLP. (default = 'softmax').\n",
    "    - dropout : float, dropout rate (default = 0.6).\n",
    "    - reg_params : tuple of floats for regularization (default = (0.01, 0.01))\n",
    "    - seed : int, Seed used for random mechanisms (default = 4347)\n",
    "    - name : str, name on which to save layer. (default = 'decoder')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int, hidden_layers: int = 2, hidden_nodes: int = 30, activation_fn='sigmoid',\n",
    "                 output_fn='softmax', dropout: float = 0.6, reg_params=(0.01, 0.01), seed: int = 4347,\n",
    "                 name: float = 'MLP'):\n",
    "=======\n",
    "    - hidden_nodes : int, For \"hidden\" feedforward layers, the dimensionality of the output space. (default = 30)\n",
    "    - activation_fn : str/fn, The activation function to use. (default = 'sigmoid')\n",
    "    - output_fn : str/fn, The activation function on the output of the MLP. (default = 'softmax').\n",
    "    - dropout : float, dropout rate (default = 0.6).\n",
    "    - regulariser_params : tuple of floats for regularization (default = (0.01, 0.01))\n",
    "    - seed : int, Seed used for random mechanisms (default = 4347)\n",
    "    - name : str, name on which to save layer. (default = 'MLP')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int, hidden_layers: int = 2, hidden_nodes: int = 30, activation_fn='sigmoid',\n",
    "                 output_fn='softmax', dropout: float = 0.6, regulariser_params: tuple = (0.01, 0.01), seed: int = 4347,\n",
    "                 name: str = 'MLP'):\n",
    ">>>>>>> develop\n",
    "\n",
    "        # Block parameters\n",
    "        super().__init__(name=name)\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.output_fn = output_fn\n",
    "\n",
    "        # Regularization params\n",
    "        self.dropout = dropout\n",
    "<<<<<<< HEAD\n",
    "        self.regulariser = mix_l1_l2_reg(reg_params)\n",
    "=======\n",
    "        self.regulariser = mix_l1_l2_reg(regulariser_params)\n",
    ">>>>>>> develop\n",
    "\n",
    "        # Seed\n",
    "        self.seed = seed\n",
    "\n",
    "        # Add intermediate layers to the model\n",
    "<<<<<<< HEAD\n",
    "        for layer_id_ in range(1, self.hidden_layers + 1):\n",
    "=======\n",
    "        for layer_id_ in range(self.hidden_layers):\n",
    ">>>>>>> develop\n",
    "            # Add Dense layer to model\n",
    "            layer_ = Dense(units=self.hidden_nodes,\n",
    "                           activation=self.activation_fn,\n",
    "                           kernel_regularizer=self.regulariser,\n",
    "                           activity_regularizer=self.regulariser)\n",
    "\n",
    "            self.__setattr__('layer_' + str(layer_id_), layer_)\n",
    "\n",
    "<<<<<<< HEAD\n",
    "        # Add Input and Output Layers\n",
    "        self.input_layer = Dense(units=self.hidden_nodes, activation=self.activation_fn)\n",
    "        self.output_layer = Dense(units=self.output_dim, activation=self.output_fn)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout_layer = Dropout(rate=self.dropout, seed=self.seed)\n",
    "\n",
    "    def call(self, inputs, training=True, **kwargs):\n",
    "=======\n",
    "            # Add corresponding Dropout Layer\n",
    "            dropout_layer = Dropout(rate=self.dropout, seed=self.seed + layer_id_)\n",
    "            self.__setattr__('dropout_layer_' + str(layer_id_), dropout_layer)\n",
    "\n",
    "        # Input and Output layers\n",
    "        self.input_layer = Dense(units=self.hidden_nodes, activation=self.activation_fn)\n",
    "        self.output_layer = Dense(units=self.output_dim, activation=self.output_fn)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    ">>>>>>> develop\n",
    "        \"\"\"Forward pass of layer block.\"\"\"\n",
    "        x_inter = self.input_layer(inputs)\n",
    "\n",
    "        # Iterate through hidden layer computation\n",
    "        for layer_id_ in range(self.hidden_layers):\n",
    "<<<<<<< HEAD\n",
    "            # Get layer and run on input\n",
    "            layer_ = self.__getattribute__('layer_' + str(layer_id_))\n",
    "            x_inter = self.dropout_layer(layer_(x_inter, training=training), seed=self.seed)\n",
    "\n",
    "        return self.output_layer(x_inter, training=training, **kwargs)\n",
    "=======\n",
    "            # Get layers\n",
    "            layer_ = self.__getattribute__('layer_' + str(layer_id_))\n",
    "            dropout_layer_ = self.__getattribute__('dropout_layer_' + str(layer_id_))\n",
    "\n",
    "            # Make layer computations\n",
    "            x_inter = dropout_layer_(layer_(x_inter, training=training))\n",
    "\n",
    "        return self.output_layer(x_inter, training=training)\n",
    ">>>>>>> develop\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Update configuration for layer.\"\"\"\n",
    "\n",
    "<<<<<<< HEAD\n",
    "        # Get overarching configuration\n",
    "        config = super().get_config().copy()\n",
    "\n",
    "        # Update configuration with parameters above\n",
    "=======\n",
    "        # Load existing configuration\n",
    "        config = super().get_config().copy()\n",
    "\n",
    "        # Update configuration\n",
    ">>>>>>> develop\n",
    "        config.update({f\"{self.name}-output_dim\": self.output_dim,\n",
    "                       f\"{self.name}-hidden_layers\": self.hidden_layers,\n",
    "                       f\"{self.name}-hidden_nodes\": self.hidden_nodes,\n",
    "                       f\"{self.name}-activation_fn\": self.activation_fn,\n",
    "                       f\"{self.name}-output_fn\": self.output_fn,\n",
    "                       f\"{self.name}-dropout\": self.dropout,\n",
    "                       f\"{self.name}-seed\": self.seed})\n",
    "\n",
    "        return config\n",
    "\n",
    "\n",
    "class FeatTimeAttention(Layer):\n",
    "    \"\"\"\n",
    "<<<<<<< HEAD\n",
    "    Feature Time Attention Layer. Computes approximations when given output RNN cell states.\n",
    "=======\n",
    "    Custom Feature Attention Layer. Features are projected to latent dimension and approximate output RNN states.\n",
    "    Approximations are sum-weighted to obtain a final representation.\n",
    ">>>>>>> develop\n",
    "\n",
    "    Params:\n",
    "    units: int, dimensionality of projection/latent space.\n",
    "    activation: str/fn, the activation function to use. (default = \"relu\")\n",
    "    name: str, the name on which to save the layer. (default = \"custom_att_layer\")\n",
    "    \"\"\"\n",
    "\n",
    "<<<<<<< HEAD\n",
    "    def __init__(self, units: int, activation=\"relu\", name: str = \"custom_layer\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        # Layer hyper-params\n",
    "=======\n",
    "    def __init__(self, units, activation=\"linear\", name: str = \"custom_layer\"):\n",
    "\n",
    "        # Load layer params\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        # Initialise key layer attributes\n",
    ">>>>>>> develop\n",
    "        self.units = units\n",
    "        self.activation_name = activation\n",
    "        self.activation = tf.keras.activations.get(activation)  # get activation from  identifier\n",
    "\n",
    "<<<<<<< HEAD\n",
    "        # Layer kernel and bias initialised to None (updated on build method)\n",
    "        self.kernel = None\n",
    "        self.bias = None\n",
    "        self.unnorm_beta = None\n",
    "\n",
    "    def build(self, input_shape=None):\n",
    "        \"\"\"Build method for initialising layers when seeing input.\"\"\"\n",
    "\n",
    "        # Initialise dimensions\n",
    "        N, T, D_f = input_shape\n",
    "\n",
    "        # Define Kernel and Bias for Feature Projection\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[1, 1, D_f, self.units],\n",
    "                                      initializer=\"glorot_uniform\", trainable=True)\n",
    "        self.bias = self.add_weight(\"bias\", shape=[1, 1, D_f, self.units],\n",
    "                                    initializer='uniform', trainable=True)\n",
    "\n",
    "        # Define Time aggregation weights for averaging over time.\n",
    "        self.unnorm_beta = self.add_weight(name='unnorm_beta_weights', shape=[1, T, 1],\n",
    "                                           initializer=\"uniform\", trainable=True)\n",
    "=======\n",
    "        # Initialise layer weights to None\n",
    "        self.kernel = None\n",
    "        self.bias = None\n",
    "        self.unnorm_beta_weights = None\n",
    "\n",
    "    def build(self, input_shape=None):\n",
    "        \"\"\"Build method for the layer given input shape.\"\"\"\n",
    "        N, T, Df = input_shape\n",
    "\n",
    "        # kernel, bias for feature -> latent space conversion\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[1, 1, Df, self.units],\n",
    "                                      initializer=\"glorot_uniform\", trainable=True)\n",
    "        self.bias = self.add_weight(\"bias\", shape=[1, 1, Df, self.units],\n",
    "                                    initializer='uniform', trainable=True)\n",
    "\n",
    "        # Time aggregation learn weights\n",
    "        self.unnorm_beta_weights = self.add_weight(name='time_agg', shape=[1, T, 1],\n",
    "                                                   initializer=\"uniform\", trainable=True)\n",
    ">>>>>>> develop\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "<<<<<<< HEAD\n",
    "        Forward pass of the Feature Time layer - requires inputs and estimated latent projections.\n",
    "\n",
    "        Params:\n",
    "        inputs: Tuple of tensors\n",
    "            Input data - Tensor of shape (batch size, T, Df)\n",
    "            Latent_reps - Tensor of shape (batch_size, T, latent dim)\n",
    "\n",
    "        Approximations to latent_reps are computed. At a second stage, approximations are sum-weighted according\n",
    "        to the beta weights.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load tuple\n",
    "        input_data, latent_reps = inputs\n",
    "\n",
    "        # Generate latent vector approximation\n",
    "        o_hat, _ = self.compute_output_state_approximations(input_data, latent_reps)  # shape (bs, T, latent_dim)\n",
    "\n",
    "        # Weighted sum over time. Weights are not normalised, hence must be converted through softmax.\n",
    "        time_weights = _norm_abs(self.unnorm_beta)  # shape (1, T, 1)\n",
    "        z = tf.reduce_sum(tf.math.multiply(o_hat, time_weights), axis=1)  # shape (bs, latent_dim)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def compute_output_state_approximations(self, inputs, latent_reps):\n",
    "        \"\"\"Given input and targets (latent_reps), compute OLS approximation to targets and weights.\"\"\"\n",
    "\n",
    "        # Compute feature projections\n",
    "        feature_linear_proj = tf.math.multiply(tf.expand_dims(inputs, axis=-1), self.kernel) + self.bias\n",
    "        feature_projections = self.activation(feature_linear_proj)\n",
    "\n",
    "        # Estimate OLS coefficients\n",
    "        alpha_t = _estimate_alpha(feature_projections, targets=latent_reps)\n",
    "\n",
    "        # Compute OLS estimates given coefficients\n",
    "=======\n",
    "        Forward pass of the Custom layer - requires inputs and estimated latent projections.\n",
    "\n",
    "        Params:\n",
    "        - inputs: tuple of two arrays:\n",
    "            - input_data: array-like of input data of shape (bs, T, D_f)\n",
    "            - latent_reps: array-like of representations of shape (bs, T, units)\n",
    "\n",
    "        returns:\n",
    "        - latent_representation (z): array-like of shape (bs, units)\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack input\n",
    "        input_data, latent_reps = inputs\n",
    "\n",
    "        # Compute output state approximations\n",
    "        o_hat, _ = self.compute_o_hat_and_alpha(input_data, latent_reps)\n",
    "\n",
    "        # Normalise temporal weights and sum-weight approximations to obtain representation\n",
    "        beta_scores = _norm_abs(self.unnorm_beta_weights)\n",
    "        z = tf.reduce_sum(tf.math.multiply(o_hat, beta_scores), axis=1)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def compute_o_hat_and_alpha(self, inputs, latent_reps):\n",
    "        \"\"\"\n",
    "        Compute approximation to latent representations, given input feature data.\n",
    "\n",
    "        Params:\n",
    "        - inputs: array-like of shape (bs, T, D_f)\n",
    "        - latent_reps: array-like of shape (bs, T, units)\n",
    "\n",
    "        returns:\n",
    "        - output: tuple of arrays:\n",
    "           - array-like of shape (bs, T, units) of representation approximations\n",
    "           - array-like of shape (bs, T, D_f) of alpha_weights\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute feature projections\n",
    "        feature_projections = self.activation(\n",
    "            tf.math.multiply(tf.expand_dims(inputs, axis=-1), self.kernel) + self.bias)\n",
    "\n",
    "        # estimate alpha coefficients through OLS\n",
    "        alpha_t = _estimate_alpha(feature_projections, targets=latent_reps)\n",
    "\n",
    "        # sum-weight feature projections according to alpha_t to compute latent approximations\n",
    ">>>>>>> develop\n",
    "        o_hat = tf.reduce_sum(tf.math.multiply(tf.expand_dims(alpha_t, axis=-1), feature_projections), axis=2)\n",
    "\n",
    "        return o_hat, alpha_t\n",
    "\n",
    "<<<<<<< HEAD\n",
    "    def compute_unnorm_scores(self, inputs, latent_reps, cluster_reps):\n",
    "        \"\"\"\n",
    "        Compute unnormalised alpha, beta, gamma scores given\n",
    "        input data\n",
    "        latent_representation\n",
    "        cluster_representations (if None, ignore gamma).\n",
    "        \"\"\"\n",
    "\n",
    "        # alpha estimated from OLS regression\n",
    "        o_hat, alpha_t = self.compute_output_state_approximations(inputs, latent_reps)  # shape bs, T, D_f\n",
    "\n",
    "        # beta estimated from beta_weights\n",
    "        beta = self.unnorm_beta  # shape (1, T, 1)\n",
    "\n",
    "        # gamma estimated from approximation to cluster reps\n",
    "        if cluster_reps is None:\n",
    "            gamma_t_k = None\n",
    "        else:\n",
    "            gamma_t_k = _estimate_gamma(o_hat, cluster_reps)  # shape (bs, K, T)\n",
    "\n",
    "        return alpha_t, beta, gamma_t_k\n",
    "\n",
    "    def compute_norm_scores(self, inputs, latent_reps, cluster_reps):\n",
    "        \"\"\"Compute normalised scores alpha, beta, gamma.\"\"\"\n",
    "\n",
    "        # Load weights\n",
    "        alpha, beta, gamma = self.compute_unnorm_scores(inputs, latent_reps, cluster_reps)\n",
    "\n",
    "        # Normalise\n",
    "        alpha = _norm_abs(alpha, axis=1)\n",
    "        beta = _norm_abs(beta, axis=1)\n",
    "\n",
    "        # Check if gamma is None\n",
    "        if gamma is None:\n",
    "            gamma = None\n",
    "        else:\n",
    "            gamma = _norm_abs(gamma, axis=1)\n",
    "\n",
    "        return alpha, beta, gamma\n",
    "=======\n",
    "    def compute_unnorm_scores(self, inputs, latent_reps, cluster_reps=None):\n",
    "        \"\"\"\n",
    "        Compute unnorm_weights for attention values.\n",
    "\n",
    "        Params: - inputs: array-like of shape (bs, T, D_f) of input data - latent_reps: array-like of shape (bs, T,\n",
    "        units) of RNN cell output states. - cluster_reps: array-like of shape (K, units) of cluster representation\n",
    "        vectors (default = None). If None, gamma computation is skipped.\n",
    "\n",
    "        Returns: - output: tuple of arrays (alpha, beta, gamma) with corresponding values. If cluster_reps is None,\n",
    "        gamma computation is skipped.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute alpha weights\n",
    "        o_hat, alpha_t = self.compute_o_hat_and_alpha(inputs, latent_reps)\n",
    "\n",
    "        # Load beta weights\n",
    "        beta = self.unnorm_beta_weights\n",
    "\n",
    "        # If cluster_reps not None, compute gamma\n",
    "        if cluster_reps is None:\n",
    "            gamma_t_k = None\n",
    "        else:\n",
    "            gamma_t_k = _estimate_gamma(o_hat, cluster_reps)\n",
    "\n",
    "        return alpha_t, beta, gamma_t_k\n",
    "\n",
    "    def compute_norm_scores(self, inputs, latent_reps, cluster_reps=None):\n",
    "        \"\"\"\n",
    "        Compute normalised attention scores alpha, beta, gamma.\n",
    "\n",
    "        Params: - inputs: array-like of shape (bs, T, D_f) of input data - latent_reps: array-like of shape (bs, T,\n",
    "        units) of RNN cell output states. - cluster_reps: array-like of shape (K, units) of cluster representation\n",
    "        vectors (default = None). If None, gamma computation is skipped.\n",
    "\n",
    "        Returns: - output: tuple of arrays (alpha, beta, gamma) with corresponding normalised scores. If cluster_reps\n",
    "        is None, gamma computation is skipped.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load unnormalised scores\n",
    "        alpha, beta, gamma = self.compute_unnorm_scores(inputs, latent_reps, cluster_reps)\n",
    "\n",
    "        # Normalise\n",
    "        alpha_norm = _norm_abs(alpha, axis=1)\n",
    "        beta_norm = _norm_abs(beta, axis=1)\n",
    "\n",
    "        if gamma is None:\n",
    "            gamma_norm = None\n",
    "        else:\n",
    "            gamma_norm = _norm_abs(gamma, axis=1)\n",
    "\n",
    "        return alpha_norm, beta_norm, gamma_norm\n",
    ">>>>>>> develop\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Update configuration for layer.\"\"\"\n",
    "\n",
    "<<<<<<< HEAD\n",
    "        # Get overarching configuration\n",
    "        config = super().get_config().copy()\n",
    "\n",
    "        # Update configuration with parameters above\n",
    "        config.update({f\"{self.name}-units\": self.units,\n",
    "                       f\"{self.name}-activation\": self.activation_name\n",
    "                       })\n",
    "=======\n",
    "        # Load existing configuration\n",
    "        config = super().get_config().copy()\n",
    "\n",
    "        # Update configuration\n",
    "        config.update({f\"{self.name}-units\": self.units,\n",
    "                       f\"{self.name}-activation\": self.activation})\n",
    ">>>>>>> develop\n",
    "\n",
    "        return config\n",
    "\n",
    "\n",
    "class LSTMEncoder(Layer):\n",
    "    \"\"\"\n",
    "        Class for a stacked LSTM layer architecture.\n",
    "\n",
    "        Params:\n",
    "        - latent_dim : dimensionality of latent space for each sub-sequence. (default = 32)\n",
    "        - hidden_layers : Number of \"hidden\"/intermediate LSTM layers.  (default = 1)\n",
    "<<<<<<< HEAD\n",
    "        - hidden_nodes : For hidden LSTM layers, the dimensionality of the intermediate state. (default = 32)\n",
    "=======\n",
    "        - hidden_nodes : For hidden LSTM layers, the dimensionality of the intermediate state. (default = 20)\n",
    ">>>>>>> develop\n",
    "        - state_fn : The activation function to use on cell state/output. (default = 'tanh')\n",
    "        - recurrent_activation : The activation function to use on forget/input/output gates. (default = 'sigmoid')\n",
    "        - return_sequences : Indicates if returns sequence of states on the last layer (default = False)\n",
    "        - dropout : dropout rate to be used on cell state/output computation. (default = 0.6)\n",
    "        - recurrent_dropout : dropout rate to be used on forget/input/output gates. (default = 0.0)\n",
    "        - regulariser_params :  tuple of floats indicating l1_l2 regularisation. (default = (0.01, 0.01))\n",
    "<<<<<<< HEAD\n",
    "        - name : Name on which to save component. (default = 'encoder')\n",
    "        - seed : seed value for dropout layer (default = 4347)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int = 32, hidden_layers: int = 1, hidden_nodes: int = 20, state_fn: str = \"tanh\",\n",
    "                 recurrent_fn: str = \"sigmoid\", regulariser_param: tuple = (0.01, 0.01), return_sequences: bool = False,\n",
    "                 dropout: float = 0.6, recurrent_dropout: float = 0.0, name: str = 'LSTM_Encoder', seed: int = 4347):\n",
    "=======\n",
    "        - name : Name on which to save component. (default = 'LSTM_Encoder')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int = 32, hidden_layers: int = 1, hidden_nodes: int = 20, state_fn=\"tanh\",\n",
    "                 recurrent_fn=\"sigmoid\", regulariser_params: tuple = (0.01, 0.01), return_sequences: bool = False,\n",
    "                 dropout: float = 0.6, recurrent_dropout: float = 0.0, name: str = 'LSTM_Encoder'):\n",
    ">>>>>>> develop\n",
    "\n",
    "        # Block Parameters\n",
    "        super().__init__(name=name)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.state_fn = state_fn\n",
    "        self.recurrent_fn = recurrent_fn\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "<<<<<<< HEAD\n",
    "        # Regularisation parameters\n",
    "        self.dropout = dropout\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.regulariser_params = regulariser_param\n",
    "        self.regulariser = mix_l1_l2_reg(regulariser_param)\n",
    "\n",
    "        # Seed\n",
    "        self.seed = seed\n",
    "\n",
    "        # Add Hidden/Intermediate Layers\n",
    "        for layer_id_ in range(1, self.hidden_layers + 1):\n",
    "=======\n",
    "        # Regularisation Params\n",
    "        self.dropout = dropout\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.regulariser_params = regulariser_params\n",
    "        self.regulariser = mix_l1_l2_reg(regulariser_params)\n",
    "\n",
    "        # Add Intermediate Layers\n",
    "        for layer_id_ in range(self.hidden_layers):\n",
    ">>>>>>> develop\n",
    "            self.__setattr__('layer_' + str(layer_id_),\n",
    "                             LSTM(units=self.hidden_nodes, return_sequences=True, activation=self.state_fn,\n",
    "                                  recurrent_activation=self.recurrent_fn, dropout=self.dropout,\n",
    "                                  recurrent_dropout=self.recurrent_dropout,\n",
    "                                  kernel_regularizer=self.regulariser, recurrent_regularizer=self.regulariser,\n",
    "                                  bias_regularizer=self.regulariser, return_state=False))\n",
    "\n",
    "<<<<<<< HEAD\n",
    "        # Add Input and Output Layers\n",
    "=======\n",
    "        # Input and Output Layers\n",
    ">>>>>>> develop\n",
    "        self.input_layer = LSTM(units=self.hidden_nodes, activation=self.state_fn,\n",
    "                                recurrent_activation=self.recurrent_fn, return_sequences=True,\n",
    "                                dropout=self.dropout, recurrent_dropout=self.recurrent_dropout,\n",
    "                                kernel_regularizer=self.regulariser, recurrent_regularizer=self.regulariser,\n",
    "                                bias_regularizer=self.regulariser, return_state=False)\n",
    "\n",
    "        self.output_layer = LSTM(units=self.latent_dim, activation=self.state_fn,\n",
    "                                 recurrent_activation=self.recurrent_fn, return_sequences=self.return_sequences,\n",
    "                                 dropout=self.dropout, recurrent_dropout=self.recurrent_dropout,\n",
    "                                 kernel_regularizer=self.regulariser, recurrent_regularizer=self.regulariser,\n",
    "                                 bias_regularizer=self.regulariser, return_state=False)\n",
    "\n",
    "    def call(self, inputs, mask=None, training=True):\n",
    "        \"\"\"Forward pass of layer block.\"\"\"\n",
    "        x_inter = self.input_layer(inputs)\n",
    "\n",
    "        # Iterate through hidden layer computation\n",
    "<<<<<<< HEAD\n",
    "        for layer_id_ in range(1, self.hidden_layers + 1):\n",
    "=======\n",
    "        for layer_id_ in range(self.hidden_layers):\n",
    ">>>>>>> develop\n",
    "            layer_ = self.__getattribute__('layer_' + str(layer_id_))\n",
    "            x_inter = layer_(x_inter, training=training)\n",
    "\n",
    "        return self.output_layer(x_inter, training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Update configuration for layer.\"\"\"\n",
    "\n",
    "<<<<<<< HEAD\n",
    "        # Get overarching configuration\n",
    "        config = super().get_config().copy()\n",
    "\n",
    "        # Update configuration with parameters above\n",
    "=======\n",
    "        # Load existing configuration\n",
    "        config = super().get_config().copy()\n",
    "\n",
    "        # Update configuration\n",
    ">>>>>>> develop\n",
    "        config.update({f\"{self.name}-latent_dim\": self.latent_dim,\n",
    "                       f\"{self.name}-hidden_layers\": self.hidden_layers,\n",
    "                       f\"{self.name}-hidden_nodes\": self.hidden_nodes,\n",
    "                       f\"{self.name}-state_fn\": self.state_fn,\n",
    "                       f\"{self.name}-recurrent_fn\": self.recurrent_fn,\n",
    "                       f\"{self.name}-return_sequences\": self.return_sequences,\n",
    "                       f\"{self.name}-dropout\": self.dropout,\n",
    "                       f\"{self.name}-recurrent_dropout\": self.recurrent_dropout,\n",
    "<<<<<<< HEAD\n",
    "                       f\"{self.name}-regulariser_params\": self.regulariser_params,\n",
    "                       f\"{self.name}-seed\": self.seed})\n",
    "=======\n",
    "                       f\"{self.name}-regulariser_params\": self.regulariser_params})\n",
    ">>>>>>> develop\n",
    "\n",
    "        return config\n",
    "\n",
    "\n",
    "class AttentionRNNEncoder(LSTMEncoder):\n",
    "    \"\"\"\n",
    "<<<<<<< HEAD\n",
    "        Class for an Attention RNN Encoder architecture.\n",
    "\n",
    "        Params:\n",
    "    units: int, dimensionality of projection/latent space.\n",
    "    activation: str/fn, the activation function to use. (default = \"relu\")\n",
    "    name: str, name on which to save model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units: int, activation: str = \"linear\", name: str = \"AttentionRNNEncoder\", **kwargs):\n",
    "        super().__init__(latent_dim=units, return_sequences=True, name=name, **kwargs)\n",
    "        self.feat_time_att_layer = FeatTimeAttention(units=units, activation=activation)\n",
    "\n",
    "    def call(self, inputs, mask=None, training=True):\n",
    "        \"\"\"Forward pass of layer block.\"\"\"\n",
    "\n",
    "        # Compute latent representations through RNN Encoder\n",
    "        latent_reps = super().call(inputs, mask=mask, training=training)\n",
    "\n",
    "        # Attention layer inputs\n",
    "        attention_inputs = inputs, latent_reps\n",
    "        z = self.feat_time_att_layer(attention_inputs)\n",
    "=======\n",
    "        Class for an Attention RNN Encoder architecture. Class builds on LSTM Encoder class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, activation=\"linear\", **kwargs):\n",
    "        super().__init__(latent_dim=units, return_sequences=True, **kwargs)\n",
    "        self.feat_time_attention_layer = FeatTimeAttention(units=units, activation=activation)\n",
    "\n",
    "    def call(self, inputs, mask=None, training: bool = True):\n",
    "        \"\"\"\n",
    "        Forward pass of layer block.\n",
    "\n",
    "        Params:\n",
    "        - inputs: array-like of shape (bs, T, D_f)\n",
    "        - mask: array-like of shape (bs, T) (default = None)\n",
    "        - training: bool indicating whether to make computation in training mode or not. (default = True)\n",
    "\n",
    "        Returns:\n",
    "        - z: array-like of shape (bs, units)\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute LSTM output states\n",
    "        latent_reps = super().call(inputs, mask=mask, training=training)\n",
    "\n",
    "        # Compute representation through feature time attention layer\n",
    "        attention_inputs = (inputs, latent_reps)\n",
    "        z = self.feature_time_att_layer(attention_inputs)\n",
    ">>>>>>> develop\n",
    "\n",
    "        return z\n",
    "\n",
    "    def compute_unnorm_scores(self, inputs, cluster_reps=None):\n",
    "<<<<<<< HEAD\n",
    "        \"\"\"\n",
    "        Compute alpha, beta, gamma un-normalised scores for cluster estimation.\n",
    "\n",
    "        Params:\n",
    "        - inputs: input data\n",
    "        - cluster_reps: set of cluster representation vectors (default = None)\n",
    "        \"\"\"\n",
    "        latent_reps = super().call(inputs, training=False)\n",
    "\n",
    "        return self.feat_time_att_layer.compute_unnorm_scores(inputs, latent_reps, cluster_reps)\n",
    "\n",
    "    def compute_norm_scores(self, inputs, cluster_reps=None):\n",
    "        \"\"\"\n",
    "        Compute alpha, beta, gamma normalised scores for cluster estimation.\n",
    "\n",
    "        Params:\n",
    "        - inputs: input data\n",
    "        - cluster reps: set of cluster representation vectors (default = None).\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute latent representations and estimate alpha\n",
    "        latent_reps = super().call(inputs, training=False)\n",
    "        alpha, beta, gamma = self.feat_time_att_layer.compute_norm_scores(inputs, latent_reps, cluster_reps)\n",
    "\n",
    "        return alpha, beta, gamma\n",
    "=======\n",
    "        \"\"\"Compute unnormalised scores alpha, beta, gamma given input data and cluster representation vectors.\n",
    "\n",
    "        Params:\n",
    "        - inputs: array-like of shape (bs, T, D_f)\n",
    "        - cluster_reps: array-like of shape (K, units) of cluster representation vectors. (default = None)\n",
    "\n",
    "        If cluster_reps is None, compute only alpha and beta weights.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple of arrays, containing alpha, beta, gamma unnormalised attention weights.\n",
    "        \"\"\"\n",
    "        latent_reps = super().call(inputs, training=False)\n",
    "\n",
    "        return self.feature_time_att_layer.compute_unnorm_scores(inputs, latent_reps, cluster_reps)\n",
    "\n",
    "    def compute_norm_scores(self, inputs, cluster_reps=None):\n",
    "        \"\"\"Compute normalised scores alpha, beta, gamma given input data and cluster representation vectors.\n",
    "\n",
    "        Params:\n",
    "        - inputs: array-like of shape (bs, T, D_f)\n",
    "        - cluster_reps: array-like of shape (K, units) of cluster representation vectors. (default = None)\n",
    "\n",
    "        If cluster_reps is None, compute only alpha and beta weights.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple of arrays, containing alpha, beta, gamma normalised attention weights.\n",
    "        \"\"\"\n",
    "        latent_reps = super().call(inputs, training=False)\n",
    "\n",
    "        return self.feature_time_att_layer.compute_norm_scores(inputs, latent_reps, cluster_reps)\n",
    ">>>>>>> develop\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Update configuration for layer.\"\"\"\n",
    "        return super().get_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdac870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras import optimizers\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Auxiliary\n",
    "\n",
    "\n",
    "\n",
    "def _class_weighting(y_true):\n",
    "    \"\"\"\n",
    "    Function to compute inverse class proportion weighting given array of true class assignments.\n",
    "\n",
    "    Params:\n",
    "    - y_true: array-like of shape (N, num_outcs) with corresponding one-hot encoding assignment of true class\n",
    "\n",
    "    Returns:\n",
    "    - weights: array-like of shape (num_outcs) with weights inversely proportional to number of true class examples.\n",
    "    \"\"\"\n",
    "\n",
    "    class_numbers = tf.reduce_sum(y_true, axis=0)\n",
    "\n",
    "    # Check no class is missing\n",
    "    if not tf.reduce_all(class_numbers > 0):\n",
    "        class_numbers += 1\n",
    "\n",
    "    # Compute reciprocal\n",
    "    inv_class_numbers = 1 / tf.reduce_sum(y_true, axis=0)\n",
    "\n",
    "    return inv_class_numbers / tf.reduce_sum(inv_class_numbers)\n",
    "\n",
    "\n",
    "class CAMELOT(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Model Class for CAMELOT architecture.\n",
    "\n",
    "    Params:\n",
    "\n",
    "        (General)\n",
    "    - num_clusters: number of clusters. (default = 10)\n",
    "    - latent_dim: dimensionality of latent space. (default = 32)\n",
    "    - output_dim: dimensionality of output space. (default = 4)\n",
    "    - seed: Seed to run analysis on. (default = 4347)\n",
    "    - name: Name to give the model. (default = \"Camelot\")\n",
    "\n",
    "        (Loss functions)\n",
    "    - alpha: weighting in cluster entropy. (default = 0.01)\n",
    "    - beta: weighting in clustering representation separation. (default = 0.01)\n",
    "\n",
    "        (Regularisation Params)\n",
    "    - regulariser_params: tuple of l1_l2 float weights. (default = (0.01, 0.01))\n",
    "    - dropout: float corresponding to dropout value. (default = 0.6)\n",
    "\n",
    "        (Encoder Params)\n",
    "    - encoder_params: Dictionary indicating parameters for Encoder architecture, as follows:\n",
    "            - activation: activation function of custom feature projection component (default = \"linear\")\n",
    "            - hidden_layers: Number of \"hidden\"/intermediate LSTM layers.  (default = 1)\n",
    "            - hidden_nodes: Dimensionality of the intermediate state computation. (default = 20)\n",
    "            - state_fn: The activation function to use on cell state/output. (default = 'tanh')\n",
    "            - recurrent_activation: The activation function to use on F/I/O gates. (default = 'sigmoid')\n",
    "            - recurrent_dropout: dropout rate to be used on forget/input/output gates. (default = 0.0)\n",
    "    Default value is {}, which resets to default parameters.\n",
    "\n",
    "        (Identifier Params)\n",
    "    - identifier_params: Dictionary indicating parameters for Identifier block, as follows:\n",
    "        - hidden_layers: int, Number of \"hidden\" feedforward layers. (default = 2)\n",
    "        - hidden_nodes: int, For hidden feedforward layers, the dimensionality of the output space. (default = 30)\n",
    "        - activation_fn: str/fn, The activation function to use. (default = 'sigmoid')\n",
    "    Default value is {\"name\": \"Identifier\"}, which resets to default parameters.\n",
    "\n",
    "        (Predictor Params)\n",
    "    - predictor_params: Dictionary indicating parameters for Predictor block, as follows:\n",
    "        - hidden_layers: int, Number of \"hidden\" feedforward layers. (default = 2)\n",
    "        - hidden_nodes: int, For hidden feedforward layers, the dimensionality of the output space. (default = 30)\n",
    "        - activation_fn: str/fn, The activation function to use. (default = 'sigmoid')\n",
    "    Default value is {\"name\": \"Predictor\"}, which resets to default parameters.\n",
    "\n",
    "        (Cluster Representation Params)\n",
    "    - cluster_rep_lr: Learning rate for update of cluster_representations. (default = 0.01)\n",
    "\n",
    "        (Others)\n",
    "    - optimizer_init: optimizer to use for initialisation training. (default = \"adam\")\n",
    "    - weighted_loss: whether to use weights on predictive clustering loss (default = \"True\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_clusters=10, latent_dim=32, seed=4347, output_dim=4, name=\"Camelot\",\n",
    "                 alpha=0.01, beta=0.01, regulariser_params=(0.01, 0.01), dropout=0.6,\n",
    "                 encoder_params=None, identifier_params=None, predictor_params=None, cluster_rep_lr=0.001,\n",
    "                 optimizer_init=\"adam\", weighted_loss=True, **kwargs):\n",
    "\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        # General params\n",
    "        self.K = num_clusters\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.seed = seed\n",
    "\n",
    "        # Loss function params\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        # Common to all Networks\n",
    "        self.regulariser = regulariser_params\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Build Networks\n",
    "        self.encoder_params = encoder_params if encoder_params is not None else {}\n",
    "        self.identifier_params = identifier_params if identifier_params is not None else {}\n",
    "        self.predictor_params = predictor_params if predictor_params is not None else {}\n",
    "\n",
    "        self.Encoder = AttentionRNNEncoder(units=self.latent_dim, dropout=self.dropout,\n",
    "                                           regulariser_params=self.regulariser, name=\"Encoder\",\n",
    "                                           **self.encoder_params)\n",
    "        self.Identifier = MLP(output_dim=self.K, dropout=self.dropout, output_fn=\"softmax\",\n",
    "                              regulariser_params=self.regulariser, seed=self.seed, name=\"Identifier\",\n",
    "                              **self.identifier_params)\n",
    "        self.Predictor = MLP(output_dim=self.output_dim, dropout=self.dropout, output_fn=\"softmax\",\n",
    "                             regulariser_params=self.regulariser, seed=self.seed, name=\"Predictor\",\n",
    "                             **self.predictor_params)\n",
    "\n",
    "        # Cluster Representation params\n",
    "        self.cluster_rep_set = tf.Variable(initial_value=tf.zeros(shape=[self.K, self.latent_dim], dtype='float32'),\n",
    "                                           trainable=True, name='cluster_rep')\n",
    "        self.cluster_rep_lr = cluster_rep_lr\n",
    "        self.cluster_opt = optimizers.Adam(learning_rate=self.cluster_rep_lr)\n",
    "\n",
    "        # Initialisation loss trackers\n",
    "        self._enc_pred_loss_tracker = None\n",
    "        self._iden_loss_tracker = None\n",
    "\n",
    "        # Others\n",
    "        self._optimizer_init = tf.keras.optimizers.get(optimizer_init)\n",
    "        self.weighted_loss = weighted_loss\n",
    "        self.loss_weights = None\n",
    "\n",
    "    # Build and Call Methods\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build method to serialise layers.\"\"\"\n",
    "        self.Encoder.build(input_shape)\n",
    "        self.Encoder.feat_time_attention_layer.build(input_shape)\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Call method for model.\n",
    "\n",
    "        Params:\n",
    "        - inputs: array-like of shape (bs, T, D_f)\n",
    "\n",
    "        Returns: tuple of arrays:\n",
    "            - y_pred: array-like of shape (bs, outcome_dim) with probability assignments.\n",
    "            - pi: array-like of shape (bs, K) of cluster probability assignments.\n",
    "        \"\"\"\n",
    "        y_pred, pi = self.forward_pass(inputs)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    # TRAINING RELATED METHODS\n",
    "    def forward_pass(self, inputs):\n",
    "        \"\"\"\n",
    "        Single forward pass given input data. Inputs are encoded. Encoded vectors pass through Identifier and assigned to clusters. Cluster representations are used to predict outcome.\n",
    "\n",
    "        Params:\n",
    "        - inputs: array-like of shape (bs, T, D_f) of input data.https://en.wikipedia.org/wiki/Schauder_basis\n",
    "\n",
    "        Returns: tuple of arrays:\n",
    "            - y_pred: array-like of shape (bs, outcome_dim) with probability assignments.\n",
    "            - pi: array-like of shape (bs, K) of cluster probability assignments.\n",
    "        \"\"\"\n",
    "\n",
    "        z = self.Encoder(inputs)\n",
    "        pi = self.Identifier(z)\n",
    "\n",
    "        # Sample from cluster assignments and assign corresponding cluster representations\n",
    "        cluster_samp = self._sample_from_probs(pi)\n",
    "        sample_representations = self._select_representations_from_sample(cluster_samp)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = self.Predictor(sample_representations)\n",
    "\n",
    "        return y_pred, pi\n",
    "\n",
    "    def _sample_from_probs(self, clus_probs):\n",
    "        \"\"\"\n",
    "        Method to sample cluster given cluster assignment probabilities and categorical sampling.\n",
    "\n",
    "        Params:\n",
    "        - clus_probs: array-like of shape (bs, K) of cluster assignment probabilities.\n",
    "\n",
    "        Returns:\n",
    "            - output: array-like of shape (bs,) with the corresponding sampled cluster.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to logits\n",
    "        logits = tf.math.log(tf.reshape(clus_probs, shape=[-1, self.K]))\n",
    "\n",
    "        # Categorical sampling\n",
    "        sample = tf.random.categorical(logits, num_samples=1, seed=self.seed)\n",
    "\n",
    "        return tf.squeeze(sample)\n",
    "\n",
    "    def _select_representations_from_sample(self, clus_samp):\n",
    "        \"\"\"\n",
    "        Method to select cluster representation vector given cluster assignment.\n",
    "\n",
    "        Params:\n",
    "        - cluster_samples: array-like of shape (bs, ) with the corresponding cluster.\n",
    "\n",
    "        Returns:\n",
    "            - output: array-like of shape (bs, latent_dim) with the corresponding cluster representation vector.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to one-hot encoding\n",
    "        idx_mask = tf.one_hot(clus_samp, depth=self.K)\n",
    "\n",
    "        # Obtain representation\n",
    "        clus_rep = tf.linalg.matmul(idx_mask, self.cluster_rep_set)\n",
    "\n",
    "        return clus_rep\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        \"\"\"\n",
    "        Method to train model.\n",
    "\n",
    "        Params:\n",
    "            - inputs: tuple of input (X, y) data:\n",
    "                - X: array-like of shape (bs, T, D_f) of input time-series feature data.\n",
    "                - y: array-like of shape (bs, num_outcs) of input outcome class data.\n",
    "\n",
    "        Method updates model weights according to loss backpropagation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack inputs\n",
    "        x, y = inputs\n",
    "\n",
    "        # Define variables for each network\n",
    "        pred_vars = [var for var in self.trainable_variables if 'Predictor' in var.name]\n",
    "        enc_id_vars = [var for var in self.trainable_variables if 'Encoder' in var.name or 'Identifier' in var.name]\n",
    "        rep_vars = self.cluster_rep_set\n",
    "\n",
    "        # Initialise GradientTape to compute gradients\n",
    "        with tf.GradientTape(watch_accessed_variables=True, persistent=True) as tape:\n",
    "            y_pred, pi = self.forward_pass(x)\n",
    "\n",
    "            if self.weighted_loss is True:\n",
    "                self.loss_weights = _class_weighting(y)\n",
    "\n",
    "            # compute losses\n",
    "            l_pred = model_utils.l_pred(y, y_pred, weights=self.loss_weights)\n",
    "            l_enc_id = model_utils.l_pred(y, y_pred, weights=self.loss_weights) + self.alpha * model_utils.l_dist(pi)\n",
    "            l_clus = model_utils.l_pred(y, y_pred, weights=self.loss_weights) + self.beta * model_utils.l_clus(rep_vars)\n",
    "\n",
    "        # Compute gradients\n",
    "        pred_grad = tape.gradient(target=l_pred, sources=pred_vars)\n",
    "        enc_id_grad = tape.gradient(target=l_enc_id, sources=enc_id_vars)\n",
    "        clus_grad = tape.gradient(target=l_clus, sources=rep_vars)\n",
    "\n",
    "        # Apply gradients\n",
    "        self.optimizer.apply_gradients(zip(pred_grad, pred_vars))\n",
    "        self.optimizer.apply_gradients(zip(enc_id_grad, enc_id_vars))\n",
    "        self.cluster_opt.apply_gradients(zip([clus_grad], [rep_vars]))\n",
    "\n",
    "        return l_pred, l_enc_id, l_clus\n",
    "        # return {'L1': L1.result(), 'L2': L2.result(), 'L3': L3.result()}\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        \"\"\"\n",
    "        Method to compute test step model.\n",
    "\n",
    "        Params:\n",
    "            - inputs: tuple of input (X, y) data:\n",
    "                - X: array-like of shape (bs, T, D_f) of input time-series feature data.\n",
    "                - y: array-like of shape (bs, num_outcs) of input outcome class data.\n",
    "        \"\"\"\n",
    "        x, y = inputs\n",
    "        y_pred, pi = self.forward_pass(x)\n",
    "\n",
    "        if self.weighted_loss is True:\n",
    "            self.loss_weights = _class_weighting(y)\n",
    "\n",
    "        # compute losses\n",
    "        l_pred = model_utils.l_pred(y, y_pred, weights=self.loss_weights)\n",
    "        l_enc_id = model_utils.l_pred(y, y_pred, weights=self.loss_weights) + self.alpha * model_utils.l_dist(pi)\n",
    "        l_clus = model_utils.l_pred(y, y_pred, weights=self.loss_weights) + self.beta * model_utils.l_clus(\n",
    "            self.cluster_rep_set)\n",
    "\n",
    "        return l_pred, l_enc_id, l_clus\n",
    "        # return {'L1': val_L1.result(), 'L2': val_L2.result(), 'L3': val_L3.result()}\n",
    "\n",
    "    # Initialisation procedure for the model\n",
    "    def initialise_model(self, data: tuple, val_data: tuple, epochs: int = 100, learning_rate: float = 0.001,\n",
    "                         batch_size: int = 64, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialisation Method for Model.\n",
    "\n",
    "        Params:\n",
    "            - data: tuple (X, y) of data to train the model.\n",
    "                - X: array-like of shape (N, T, D_f)\n",
    "                - y: array-like of shape (N, num_outcs)\n",
    "            - val_data: tuple (X, y) of validation data to see loss performance.\n",
    "                - X: array-like of shape (N', T, D_f)\n",
    "                - y: array-like of shape (N', num_outcs)\n",
    "            - epochs: int, number of epochs for training. (default = 100)\n",
    "            - learning_rate: float, starting learning rate for initialisation training\n",
    "            - batch_size: int, size of individual batches.\n",
    "            - kwargs: dictionary arguments for KMeans initialisation.\n",
    "\n",
    "        Updates model according to initialisation procedure. Initialisation consists of 3 steps:\n",
    "        - Outcome prediction by applying Predictor network directly on input data representation..\n",
    "        - Cluster representation initialisation through K-Means on input data representation.\n",
    "        - Identifier initialisation by minimising loss on clusters as predicted by KMeans.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "        val_x, val_y = val_data\n",
    "\n",
    "        # Compute loss weights if necessary\n",
    "        if self.weighted_loss is True:\n",
    "            self.loss_weights = _class_weighting(y)\n",
    "\n",
    "        # Initialise init learning rate\n",
    "        self._optimizer_init.learning_rate = learning_rate\n",
    "\n",
    "        # Go through initialisation steps\n",
    "        self._initialise_enc_pred(data=data, val_data=val_data, epochs=epochs, batch_size=batch_size)\n",
    "        clus_train_y, clus_val_y = self._initialise_clus(x, val_x, **kwargs)\n",
    "\n",
    "        # Initialise Identifier\n",
    "        iden_train_data = (x, clus_train_y)\n",
    "        iden_val_data = (val_x, clus_val_y)\n",
    "        self._initialise_iden(data=iden_train_data, val_data=iden_val_data,\n",
    "                              epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    def _initialise_enc_pred(self, data, val_data, epochs=100, batch_size=64):\n",
    "        \"\"\"\n",
    "          Initialisation Method for Encoder and Predictor blocks.\n",
    "\n",
    "          Params:\n",
    "              - data: tuple (X, y) of data to train the model.\n",
    "                  - X: array-like of shape (N, T, D_f)\n",
    "                  - y: array-like of shape (N, num_outcs)\n",
    "              - val_data: tuple (X, y) of validation data to see loss performance.\n",
    "                  - X: array-like of shape (N', T, D_f)\n",
    "                  - y: array-like of shape (N', num_outcs)\n",
    "              - epochs: int, number of epochs for training. (default = 100)\n",
    "              - batch_size: int, size of individual batches.\n",
    "\n",
    "        Input data passes through Encoder network to obtain data representations. Predictor then outputs a predicted class. This is matched against the true class.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack inputs\n",
    "        x, y = data\n",
    "        x_val, y_val = val_data\n",
    "\n",
    "        # Convert to tf.Dataset for iteration\n",
    "        inputs = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(buffer_size=5000).batch(batch_size)\n",
    "\n",
    "        # Initialise loss tracker\n",
    "        self._enc_pred_loss_tracker = pd.DataFrame(data=np.nan, index=range(epochs), columns=[\"train_loss\", \"val_loss\"])\n",
    "        enc_pred_vars = [var for var in self.trainable_variables if \"Encoder\" in var.name or \"Predictor\" in var.name]\n",
    "\n",
    "        # Iterate through epochs and batches\n",
    "        print(\"-\" * 20, \"\\n\", \"Initialising encoder-predictor training.\")\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            epoch_loss = 0\n",
    "            for step_, (x_batch, y_batch) in enumerate(inputs):\n",
    "                # One Training Step\n",
    "                with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                    tape.watch(enc_pred_vars)\n",
    "\n",
    "                    # Prediction and loss\n",
    "                    y_pred = self.Predictor(self.Encoder(x_batch))\n",
    "                    loss_batch = model_utils.l_pred(y_batch, y_pred, weights=self.loss_weights)\n",
    "\n",
    "                # Update gradients\n",
    "                enc_pred_grad = tape.gradient(loss_batch, enc_pred_vars)\n",
    "                self._optimizer_init.apply_gradients(zip(enc_pred_grad, enc_pred_vars))\n",
    "\n",
    "                # Update loss\n",
    "                epoch_loss += loss_batch\n",
    "\n",
    "                # Print current batch loss - clears line and re-writes\n",
    "                print(\"Batch Loss %.4f\" % loss_batch, end=\"\\r\", flush=True)\n",
    "\n",
    "            # Compute validation loss on validation data\n",
    "            y_val_pred = self.Predictor(self.Encoder(x_val))\n",
    "            loss_val = model_utils.l_pred(y_val, y_val_pred, weights=self.loss_weights)\n",
    "\n",
    "            # Print result and update tracker\n",
    "            print(\"End of epoch %d - \\n Training loss: %.4f  Validation loss %.4f\" % (\n",
    "            epoch, epoch_loss / step_, loss_val))\n",
    "            self._enc_pred_loss_tracker.loc[epoch, :] = [epoch_loss / step_, loss_val]\n",
    "\n",
    "            # Check if has improved or not\n",
    "            if self._enc_pred_loss_tracker.iloc[-50:, -1].le(loss_val + 0.001).any():\n",
    "                break\n",
    "\n",
    "    def _initialise_clus(self, x, val_x, **kwargs):\n",
    "        \"\"\"\n",
    "          Initialisation Method for cluster representation\n",
    "\n",
    "          Params:\n",
    "              - x: array-like of shape (N, T, D_f)\n",
    "              - val_x: array-like of shape (N', T, D_f)\n",
    "              - kwargs: other arguments relevant to KMeans method.\n",
    "\n",
    "        Cluster representations are initialised through KMeans on the set of data representations.\n",
    "\n",
    "        Returns:\n",
    "            - tuple:\n",
    "                - clus_train_y: array-like of shape (N, num_clus) of cluster one-hot assignments.\n",
    "                - clus_val_y: array-like of shape (N', num_clus) of cluster one-hot assignments.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute Latent Projections\n",
    "        print(\"-\" * 20, \"\\n\", \"Initialising cluster representations.\")\n",
    "        z = self.Encoder(x).numpy()\n",
    "\n",
    "        # Fit KMeans\n",
    "        km = KMeans(n_clusters=self.K, init=\"k-means++\", random_state=self.seed, **kwargs)\n",
    "        km.fit(z)\n",
    "        print(\"KMeans fit has completed.\")\n",
    "\n",
    "        # Centers are figureholders for representations and\n",
    "        centers = km.cluster_centers_\n",
    "        cluster_pred = km.predict(z)\n",
    "\n",
    "        # Compute Initialised Estimates\n",
    "        print(\"\\nInitialised Phenotypes: \", self.Predictor(centers))\n",
    "        print(\"\\nEstimated Cluster distribution: \", np.unique(cluster_pred, return_counts=True))\n",
    "\n",
    "        # Initialise embeddings and convert to one-hot encoding for Identifier\n",
    "        self.cluster_rep_set.assign(tf.convert_to_tensor(centers, name='cluster_rep', dtype='float32'))\n",
    "        clus_train_y = np.eye(self.K)[cluster_pred]\n",
    "\n",
    "        # Make predictions on validation data\n",
    "        z_val = self.Encoder(val_x).numpy()\n",
    "        clus_val_y = np.eye(self.K)[km.predict(z_val)]\n",
    "\n",
    "        return clus_train_y.astype(np.float32), clus_val_y.astype(np.float32)\n",
    "\n",
    "    def _initialise_iden(self, data, val_data, epochs=100, batch_size=64):\n",
    "        \"\"\"\n",
    "          Initialisation Method for Identifier Network\n",
    "\n",
    "          Params:\n",
    "              - data: tuple (X, clus_train_y) of data to train the model.\n",
    "                  - X_train: array-like of shape (N, T, D_f)\n",
    "                  - clus_train_y: array-like of shape (N, K)\n",
    "              - val_data: tuple (X, clus_val_y) of validation data to see loss performance.\n",
    "                  - X_train: array-like of shape (N', T, D_f)\n",
    "                  - clus_val_y: array-like of shape (N', K)\n",
    "              - epochs: int, number of epochs for training. (default = 100)\n",
    "              - batch_size: int, size of individual batches.\n",
    "\n",
    "        Input data passes through Encoder network to obtain data representations. Predictor then outputs a predicted class. This is matched against the true class.\n",
    "        \"\"\"\n",
    "        # Input in the right format\n",
    "        X_train, clus_train_y = data\n",
    "        X_val, clus_val_y = val_data\n",
    "\n",
    "        # Convert to tf.Dataset for iteration\n",
    "        inputs = tf.data.Dataset.from_tensor_slices((X_train, clus_train_y)).shuffle(buffer_size=5000).batch(batch_size)\n",
    "\n",
    "        # Initialise loss tracker\n",
    "        self._iden_loss_tracker = pd.DataFrame(data=np.nan, index=range(epochs), columns=[\"train_loss\", \"val_loss\"])\n",
    "        iden_vars = [var for var in self.trainable_variables if \"Identifier\" in var.name]\n",
    "\n",
    "        # Forward Identifier pass and train\n",
    "        print(\"-\" * 20, \"\\nInitialising Identifier training.\")\n",
    "\n",
    "        for epoch in range(epochs):  # Iterate through epochs\n",
    "            epoch_loss = 0\n",
    "            for step_, (x_batch, clus_batch) in enumerate(inputs):  # Iterate through batch\n",
    "\n",
    "                # One Training Step\n",
    "                with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                    tape.watch(iden_vars)\n",
    "\n",
    "                    # Prediction and loss\n",
    "                    clus_pred = self.Identifier(self.Encoder(x_batch))\n",
    "                    loss_batch = model_utils.l_pred(clus_batch, clus_pred)\n",
    "\n",
    "                # Update gradients\n",
    "                iden_grad = tape.gradient(loss_batch, iden_vars)\n",
    "                self._optimizer_init.apply_gradients(zip(iden_grad, iden_vars))\n",
    "\n",
    "                # Update loss\n",
    "                epoch_loss += loss_batch\n",
    "\n",
    "                # Print current batch loss - clears line and re-writes\n",
    "                print(\"Batch Loss %.4f\" % loss_batch, end=\"\\r\", flush=True)\n",
    "\n",
    "            # Compute validation loss on validation data\n",
    "            clus_val_pred = self.Identifier(self.Encoder(X_val))\n",
    "            loss_val = model_utils.l_pred(clus_val_y, clus_val_pred)\n",
    "\n",
    "            # Print result and update tracker\n",
    "            print(\"End of epoch %d - \\n Training loss: %.4f  Validation loss %.4f\" % (\n",
    "            epoch, epoch_loss / step_, loss_val))\n",
    "            self._iden_loss_tracker.loc[epoch, :] = [epoch_loss / step_, loss_val]\n",
    "\n",
    "            # Check if has improved or not - look at last 50 epoch validation loss and check if \n",
    "            if self._iden_loss_tracker.iloc[-50:, -1].le(loss_val + 0.001).any():\n",
    "                break\n",
    "\n",
    "    # USEFUL METHODS\n",
    "\n",
    "    def compute_unnorm_attention_weights(self, inputs):\n",
    "        \"\"\"\n",
    "        Computes unnormalised attention weights alpha, beta, gamma.\n",
    "\n",
    "        Params:\n",
    "        - inputs: array-like of shape (N, T, D_f).\n",
    "\n",
    "        Return: tuple of unnormalised attention weights.\n",
    "            - alpha: array-like of shape (N, T, D_f)\n",
    "            - beta: array-like of shape (1, T, 1)\n",
    "            - gamma: array-like of shape (N, K, D_f)\n",
    "        \"\"\"\n",
    "        scores = self.Encoder.compute_unnorm_scores(inputs, cluster_reps=self.cluster_rep_set)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def compute_norm_attention_weights(self, inputs):\n",
    "        \"\"\"\n",
    "        Computes normalised attention weights alpha, beta, gamma.\n",
    "\n",
    "        Params:\n",
    "        - inputs: array-like of shape (N, T, D_f).\n",
    "\n",
    "        Return: tuple of normalised attention weights.\n",
    "            - alpha: array-like of shape (N, T, D_f)\n",
    "            - beta: array-like of shape (1, T, 1)\n",
    "            - gamma: array-like of shape (N, K, D_f)\n",
    "        \"\"\"\n",
    "        scores = self.Encoder.compute_norm_scores(inputs, cluster_reps=self.cluster_rep_set)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def compute_cluster_phenotypes(self):\n",
    "        \"\"\"\n",
    "        Compute Cluster Phenotypes given cluster representations.\n",
    "        \"\"\"\n",
    "        phens = self.Predictor(self.cluster_rep_set).numpy()\n",
    "\n",
    "        return phens\n",
    "\n",
    "    def clus_assign(self, X):\n",
    "        \"\"\"\n",
    "        Compute cluster assignments given input data X.\n",
    "\n",
    "        Params:\n",
    "        - X: array-like of shape (bs, T, D_f)\n",
    "\n",
    "        Returns:\n",
    "        - clus_pred: array-like of shape (bs, ) with corresponding cluster assignment.\n",
    "            \"\"\"\n",
    "        pi = self.Identifier(self.Encoder(X)).numpy()\n",
    "        clus_pred = np.argmax(pi, axis=1)\n",
    "\n",
    "        return clus_pred\n",
    "\n",
    "    def get_cluster_reps(self):\n",
    "        return self.cluster_rep_set.numpy()\n",
    "\n",
    "    def compute_pis(self, X):\n",
    "        \"\"\"Obtain cluster assignment probabilities.\"\"\"\n",
    "        pis = self.Identifier(self.Encoder(X))\n",
    "\n",
    "        return pis.numpy()\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Update configuration for layer.\"\"\"\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\"latent_dim\": self.latent_dim, \"hidden_layers\": self.hidden_layers,\n",
    "                       \"hidde_nodes\": self.hidden_nodes, \"state_fn\": self.state_fn,\n",
    "                       \"recurrent_fn\": self.recurrent_fn, \"return_sequences\": self.return_sequences,\n",
    "                       \"dropout\": self.dropout, \"recurrent_dropout\": self.recurrent_dropout,\n",
    "                       \"regulariser_params\": self.regulariser_params})\n",
    "        \"\"\"Update configuration for layer.\"\"\"\n",
    "\n",
    "        # Load existing configuration\n",
    "        config = super().get_config().copy()\n",
    "\n",
    "        # Update configuration\n",
    "        config.update({f\"{self.name}-K\": self.K,\n",
    "                       f\"{self.name}-latent_dim\": self.latent_dim,\n",
    "                       f\"{self.name}-output_dim\": self.output_dim,\n",
    "                       f\"{self.name}-seed\": self.seed,\n",
    "                       f\"{self.name}-alpha\": self.alpha,\n",
    "                       f\"{self.name}-beta\": self.beta,\n",
    "                       f\"{self.name}-regulariser\": self.regulariser,\n",
    "                       f\"{self.name}-dropout\": self.dropout,\n",
    "                       f\"{self.name}-weighted_loss\": self.weighted_loss})\n",
    "\n",
    "        return config\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    Class for fitting and evaluating Camelot architecture model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_info, model_config):\n",
    "        \"Initialise model configuration parameters.\"\n",
    "        self.data_info = data_info\n",
    "        self.model_config = model_config\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, train_params):\n",
    "        \"\"\"\n",
    "        Fit method for training CAMELOT model.\n",
    "\n",
    "        Params:\n",
    "        - train_params: dictionary containing trianing parameter information:\n",
    "            - \"lr\": learning rate for training\n",
    "            - \"epochs_init\": number of epochs to train initialisation\n",
    "            - \"epochs\": number of epochs for main training\n",
    "            - \"bs\": batch size\n",
    "            - \"cbck_str\": callback_string indicating which callbacks to print during training\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack relevant data information\n",
    "        X_train, X_val, X_test = self.data_info[\"X\"]\n",
    "        y_train, y_val, y_test = self.data_info[\"y\"]\n",
    "        output_dim = self.data_info[\"output_dim\"]\n",
    "\n",
    "        # Unpack training parameters\n",
    "        lr = train_params[\"lr\"]\n",
    "        epochs = train_params[\"epochs\"]\n",
    "        bs = train_params[\"bs\"]\n",
    "        callback_str = train_params[\"cbck_str\"]\n",
    "\n",
    "        # TODO- ADD GPU USAGE\n",
    "\n",
    "        # Initialise model\n",
    "        self.model = CAMELOT(output_dim=output_dim, **self.model_config)\n",
    "        self.model.build(X_train.shape)\n",
    "\n",
    "        # Load optimizer\n",
    "        optimizer = optimizers.Adam(learning_rate=lr)\n",
    "        self.model.compile(optimizer=optimizer, run_eagerly=True)\n",
    "\n",
    "        # Train model on initialisation procedure\n",
    "        epochs_init = 100\n",
    "        print(\"-\" * 20, \"\\n\", \"Initialising Model\", sep=\"\\n\")\n",
    "        self.model.initialise_model(data=(X_train, y_train), val_data=(X_val, y_val), epochs=epochs, learning_rate=lr,\n",
    "                                    batch_size=bs)\n",
    "\n",
    "        # Main Training phase\n",
    "        print(\"-\" * 20, \"\\n\", \"STARTING MAIN TRAINING PHASE\")\n",
    "        callbacks, run_num = model_utils.get_callbacks(track_loss=\"L1\", other_cbcks=callback_str, early_stop=True,\n",
    "                                                       lr_scheduler=True,\n",
    "                                                       tensorboard=True)\n",
    "\n",
    "        self.model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=bs, epochs=epochs, verbose=1,\n",
    "                       callbacks=callbacks)\n",
    "\n",
    "    def evaluate(self, data_test):\n",
    "        \"\"\"\n",
    "        Evaluation method for computing output results.\n",
    "\n",
    "        Params:\n",
    "        - data_test: tuple of array-like test data:\n",
    "            - X_test: of shape (?, T, D_f)\n",
    "            - y_test: of shape (?, num_outcs)\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack test data\n",
    "        X_test, y_test = data_test\n",
    "\n",
    "        # Source outcome names and patient id info\n",
    "        id_info = self.data_info[\"ids\"][-1]\n",
    "        pat_ids = id_info[:, 0, 0]\n",
    "        outc_dims = self.data_info[\"outc_dims\"]\n",
    "\n",
    "        # Other useful defs\n",
    "        K = self.model.K\n",
    "        cluster_names = list(range(1, K + 1))\n",
    "\n",
    "        # Firstly, compute predicted y estimates\n",
    "        y_pred = pd.DataFrame(self.model.predict(X_test).numpy(), index=pat_ids, columns=outc_dims)\n",
    "        outc_pred = pd.Series(np.argmax(y_pred, axis=-1), index=pat_ids)\n",
    "        y_true = pd.Series(y_test, index=pat_ids)\n",
    "\n",
    "\n",
    "        # Secondly, compute predicted cluster assignments\n",
    "        pis_pred = pd.DataFrame(self.model.compute_pis(X_test).numpy(), index=pat_ids, columns=cluster_names)\n",
    "        clus_pred = pd.Series(self.model.clus_assign(X_test).numpy(), index=pat_ids)\n",
    "\n",
    "        # Thirdly, compute cluster phenotype information\n",
    "        clus_phenotypes = pd.DataFrame(self.model.compute_cluster_phenotypes(), index=cluster_names, columns=outc_dims)\n",
    "        cluster_rep_set = self.model.get_cluster_reps()\n",
    "\n",
    "        # Fourth, save model init losses\n",
    "        init_loss_1 = self.model._enc_pred_loss_tracker\n",
    "        init_loss_2 = self.model._iden_loss_tracker\n",
    "        enc_pred_tracker.index.name, iden_tracker.index.name = \"epoch\", \"epoch\"\n",
    "\n",
    "\n",
    "        # Save data\n",
    "        y_pred.to_csv(save_fd + \"y_pred.csv\", index=True, header=True)\n",
    "        outc_pred.to_csv(save_fd + \"outc_pred.csv\", index=True, header=True)\n",
    "        y_true.to_csv(save_fd + \"y_true.csv\", index=True, header=True)\n",
    "        pis_pred.to_csv(save_fd + \"pis_pred.csv\", index=True, header=True)\n",
    "        clus_pred.to_csv(save_fd + \"clus_pred.csv\", index=True, header=True)\n",
    "        clus_phenotypes.to_csv(save_fd + \"clus_phenotypes.csv\", index=True, header=True)\n",
    "        np.save(save_fd + \"cluster_representations.npy\", cluster_rep_set, allow_pickle=True)\n",
    "\n",
    "        # save losses and model params\n",
    "        init_loss_1.to_csv(track_fd + \"enc_pred_init_loss.csv\", index=True, header=True)\n",
    "        init_loss_2.to_csv(track_fd + \"iden_init_loss.csv\", index=True, header=True)\n",
    "\n",
    "\n",
    "        # save parasm\n",
    "        # Save Model Configuration on both results and experiments\n",
    "        # with open(save_fd + \"config\", \"w+\") as f:\n",
    "        #     json.dump(vars(params), f)\n",
    "        #     f.close()\n",
    "\n",
    "        # with open(track_fd + \"config\", \"w+\") as f:\n",
    "        #     json.dump(vars(params), f)\n",
    "        #     f.close()\n",
    "\n",
    "        # Evaluate results\n",
    "        # auc, f1, rec = os.system(\"\"\"python run_model.py --K {} --latent_dim {} --seed {}\"\"\".format(\n",
    "        #     K, latent_dim, seed))\n",
    "\n",
    "        # ------------------------------------------------------------------------------\n",
    "        \"\"\"Finally, print some basic statistics for analysing this run\"\"\"\n",
    "        y_true = y_test\n",
    "        y_pred = y_pred.values\n",
    "        # auc, f1, rec, pur = utils.super_scores(y_true, y_pred)\n",
    "        # sil, sil_avg, dbi, dbi_avg, vri, vri_avg = utils.unsuper_scores(X_test, clusters_pred)\n",
    "\n",
    "        # print(\"Supervised Performance:\", f\"AUC: {auc:.2f}\", f\"f1: {f1:.2f}\", f\"rec: {rec:.2f}\", f\"pur: {pur:.2f}\", sep = \"\\n\")\n",
    "        # print(\"Unsupervised Scores:\", f\"SIL: {sil:.2f}, {sil_avg:.2f}\", f\"DBI: {dbi:.2f}, {dbi_avg:.2f}\",\n",
    "        #       f\"vri: {vri:.2f} {vri_avg:.2f}\", sep = \"\\n\")\n",
    "\n",
    "        cluster_dist = pd.Series(data=0, index=cluster_names)\n",
    "        for clus in cluster_names:\n",
    "            cluster_dist.loc[clus] = np.sum(clusters_pred == clus)\n",
    "\n",
    "        print(\"Cluster Assignment distribution: \", cluster_dist, sep=\"\\n\")\n",
    "        print(\"Num Clusters with patients: {}\".format(np.sum(cluster_dist != 0)))\n",
    "\n",
    "        outcome_df = pd.read_csv(\"data/HAVEN/processed/copd_outcomes.csv\", index_col=0)\n",
    "        outcomes = outcome_df.loc[clusters_pred.index, :]\n",
    "\n",
    "        for clus in cluster_names:\n",
    "            print(\"Outcome distribution in Cluster {}\".format(clus))\n",
    "\n",
    "            outcome_distribution = outcomes[clusters_pred == clus].sum(axis=0)\n",
    "\n",
    "            print(outcome_distribution)\n",
    "\n",
    "        alpha_sc, beta_sc, gamma_sc = model.compute_attention_rnn_encoder_scores(X_test)\n",
    "        np.savez(save_fd + \"attention-all.npz\", alpha=alpha_sc, beta=beta_sc, gamma=gamma_sc)\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01876ca8",
   "metadata": {},
   "source": [
    "Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c878849",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_config={\n",
    "   \"lr_init\": 0.002,\n",
    "   \"lr\": 0.001,\n",
    "   \"epochs_init_1\": 30,\n",
    "   \"epochs_init_2\": 30,\n",
    "   \"epochs\": 30,\n",
    "   \"bs\": 64,\n",
    "   \"cbck_str\": \"\",\n",
    "   \"patience_epochs\": 200,\n",
    "   \"gpu\": 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0adf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Data Loading.\"\n",
    "data_info = data_loader(**data_config)\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------- Loading and Training Model -----------------------------\n",
    "\n",
    "\"Load model and fit\"\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "model = model_utils.get_model_from_str(data_info=data_info, model_config=model_config,\n",
    "                                       training_config=training_config)\n",
    "# Train model\n",
    "history = model.train(data_info=data_info, **training_config)\n",
    "\n",
    "\"Compute results on test data\"\n",
    "outputs_dic = model.analyse(data_info)\n",
    "print(outputs_dic.keys())\n",
    "\n",
    "# -------------------------------------- Evaluate Scores --------------------------------------\n",
    "\n",
    "\"Evaluate scores on the resulting models. Note X_test is converted back to input dimensions.\"\n",
    "scores = evaluate(**outputs_dic, data_info=data_info, avg=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5108a",
   "metadata": {},
   "source": [
    "Results comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Union, List\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, average_precision_score\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "\n",
    "def get_clus_outc_numbers(y_true: np.ndarray, clus_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute contingency matrix: entry (i,j) denotes the number of patients with true sample i and predicted clus j.\n",
    "\n",
    "    Params:\n",
    "    - y_true: array-like of true outcome one-hot assignments.\n",
    "    - clus_pred: array-like of cluster label assignments.\n",
    "\n",
    "    Returns:\n",
    "        - cont_matrix: numpy ndarray of shape (num_outcs, num_clus) where entry (i,j) denotes the number of patients\n",
    "        with outcome i and cluster j.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to categorical\n",
    "    labels_true = np.argmax(y_true, axis=1)\n",
    "    labels_pred = clus_pred\n",
    "\n",
    "    # Compute contingency matrix\n",
    "    cont_matrix = contingency_matrix(labels_true, labels_pred)\n",
    "\n",
    "    return cont_matrix\n",
    "\n",
    "\n",
    "def _convert_to_one_hot_from_probs(array_pred: Union[np.ndarray, pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    Convert array of predicted class/cluster probability assignments to one-hot encoding of the most common class/clus.\n",
    "\n",
    "    Params: - array_pred: array-like of shape (N, K), where K is the number of target classes, with probability class\n",
    "    assignments.\n",
    "\n",
    "    Returns:\n",
    "    - Output: array-like of shape (N, K) with one-hot encoded most likely class assignments.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to array if necessary\n",
    "    if isinstance(array_pred, pd.DataFrame):\n",
    "        array_pred = array_pred.values\n",
    "\n",
    "    # Compute dimensionality\n",
    "    if len(array_pred.shape) == 2:\n",
    "        _, K = array_pred.shape\n",
    "\n",
    "        # Convert to categorical\n",
    "        class_pred = np.eye(K)[np.argmax(array_pred, axis=1)]\n",
    "\n",
    "    else:\n",
    "        # Array_pred already categorical\n",
    "        K = array_pred.size\n",
    "\n",
    "        # Convert to categorical\n",
    "        class_pred = np.eye(K)[array_pred]\n",
    "\n",
    "    return class_pred\n",
    "\n",
    "\n",
    "def purity(y_true: np.ndarray, clus_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes Purity Score from predicted and true outcome labels. Purity Score is an external cluster validation tool\n",
    "    which computes the largest number of individuals from a given class in a given cluster, and consequently averages\n",
    "    this values over the number of clusters.\n",
    "\n",
    "    Params:\n",
    "    - y_true: array-like of shape (N, num_outcs) of true outcome labels in one-hot encoded format.\n",
    "    - clus_pred: array-like of shape (N, num_clus) of predicted outcome cluster assignments.\n",
    "\n",
    "    Returns:\n",
    "    - purity_score: float indicating purity score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert clus_pred to categorical cluster assignments\n",
    "    cm = get_clus_outc_numbers(y_true, clus_pred)  # shape (num_outcs, num_clus)\n",
    "\n",
    "    # Number of most common class in each cluster\n",
    "    max_class_numbers = np.amax(cm, axis=0)\n",
    "\n",
    "    # Compute average\n",
    "    purity_score = np.sum(max_class_numbers) / np.sum(cm)\n",
    "\n",
    "    return purity_score\n",
    "\n",
    "\n",
    "def compute_supervised_scores(y_true: np.ndarray, y_pred: np.ndarray, avg=None, outc_names=None):\n",
    "    \"\"\"\n",
    "    Compute set of supervised classification scores between y_true and y_pred. List of metrics includes:\n",
    "    a) AUROC, b) Recall, c) F1, d) Precision, e) Adjusted Rand Index and f) Normalised Mutual Information Score.\n",
    "\n",
    "    Params:\n",
    "    - y_true: array-like of shape (N, num_outcs) of one-hot encoded true class membership.\n",
    "    - y_pred: array-like of shape (N, num_outcs) of predicted outcome probability assignments.\n",
    "    - avg: parameter for a), b), c) and d) computation indicating whether class scores should be averaged, and how.\n",
    "    (default = None, all scores reported).\n",
    "    - outc_names: List or None, name of outcome dimensions.\n",
    "\n",
    "    Returns:\n",
    "        - Dictionary of performance scores:\n",
    "            - \"ROC-AUC\": list of AUROC One vs Rest values.\n",
    "            - \"Recall\": List of Recall One vs Rest values.\n",
    "            - \"F1\": List of F1 score One vs Rest values.\n",
    "            - \"Precision\": List of Precision One vs Rest values.\n",
    "            - \"ARI\": Float value indicating Adjusted Rand Index performance.\n",
    "            - \"NMI\": Float value indicating Normalised Mutual Information Score performance.\n",
    "    \"\"\"\n",
    "    num = 10000\n",
    "\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "\n",
    "\n",
    "    # # Compute AUROC and AUPRC, both custom and standard\n",
    "    # auroc, auprc = bin_utils.custom_auc_auprc(y_true, y_pred, mode=\"OvR\", num=num).values()\n",
    "    # auroc_custom, auprc_custom = bin_utils.custom_auc_auprc(y_true, y_pred, mode=\"custom\", num=num).values()\n",
    "\n",
    "    # # GET ROC AND PRC CURVES\n",
    "    # roc_prc_curves = {\n",
    "    #     \"OvR\": bin_utils.plot_auc_auprc(y_true, y_pred, mode=\"OvR\", outc_names=outc_names, num=num),\n",
    "    #     \"Custom\": bin_utils.plot_auc_auprc(y_true, y_pred, mode=\"custom\", outc_names=outc_names, num=num)\n",
    "    # }\n",
    "    auroc = roc_auc_score(y_true, y_pred, average=None)\n",
    "    \n",
    "    # Convert input arrays to categorical labels\n",
    "    labels_true, labels_pred = np.argmax(y_true, axis=1), np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Compute F1\n",
    "    f1 = f1_score(labels_true, labels_pred, average=avg)\n",
    "\n",
    "    # Compute Recall\n",
    "    rec = recall_score(labels_true, labels_pred, average=avg)\n",
    "\n",
    "    # Compute Precision\n",
    "    prec = precision_score(labels_true, labels_pred, average=avg)\n",
    "\n",
    "    # Compute ARI\n",
    "    ari = adjusted_rand_score(labels_true, labels_pred)\n",
    "\n",
    "    # Compute NMI\n",
    "    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n",
    "\n",
    "    # Compute Confusion matrix\n",
    "    cm = confusion_matrix(y_true=labels_true, y_pred=labels_pred, labels=None, sample_weight=None, normalize=None)\n",
    "\n",
    "    # Return Dictionary\n",
    "    scores_dic = {\n",
    "        \"ROC-AUC\": auroc,\n",
    "        # \"ROC-PRC\": auprc,\n",
    "        # # \"ROC-AUC-custom\": auroc_custom,\n",
    "        # # \"ROC-PRC-custom\": auprc_custom,\n",
    "        \"F1\": f1,\n",
    "        \"Recall\": rec,\n",
    "        \"Precision\": prec,\n",
    "        \"ARI\": ari,\n",
    "        \"NMI\": nmi\n",
    "    }\n",
    "\n",
    "    # return scores_dic, cm, roc_prc_curves\n",
    "    return scores_dic, cm, None\n",
    "\n",
    "\n",
    "def compute_from_eas_scores(y_true: np.ndarray, scores: np.ndarray, outc_names: np.ndarray = None, **kwargs) -> dict:\n",
    "    \"\"\"\n",
    "    Compute supervised performance metrics given input array scores.\n",
    "\n",
    "\n",
    "    Params:\n",
    "    - y_true: array-like of shape (N, num_outcs).\n",
    "    - scores: array-like of shape (N, ).\n",
    "    - outc_names: array-like of shape (num_outcs, ) with particular outcome names.\n",
    "    - kwargs: any other arguments. They are kept for coherence.\n",
    "\n",
    "    Returns:\n",
    "    - dict with scores ROC-AUC, F1, Recall, Precision per class\n",
    "    \"\"\"\n",
    "\n",
    "    # Useful info\n",
    "    num_outcs = y_true.shape[-1]\n",
    "\n",
    "    if outc_names is None:\n",
    "        outc_names = range(num_outcs)\n",
    "\n",
    "    # Useful info and initialise output\n",
    "    SCORE_NAMES = {\"ROC-AUC\": roc_auc_score, \"F1\": f1_score, \"Recall\": recall_score, \"Precision\": precision_score}\n",
    "    output_dic = {}\n",
    "\n",
    "    # Convert to useful format\n",
    "    if isinstance(scores, pd.Series) or isinstance(scores, pd.DataFrame):\n",
    "        scores = scores.values.reshape(-1)\n",
    "\n",
    "    # Convert scores to probability thresholds\n",
    "    scores_max = np.max(scores)\n",
    "    scores = scores / scores_max\n",
    "\n",
    "    # Iterate through the 4 binary scores\n",
    "    for score_name, score_fn in SCORE_NAMES.items():\n",
    "\n",
    "        # Get scoring fn\n",
    "        scoring_fn = SCORE_NAMES[score_name]\n",
    "        output_dic[score_name] = []\n",
    "\n",
    "        # Iterate over outcomes\n",
    "        for outc_id, outc in enumerate(outc_names):\n",
    "\n",
    "            # Compute score for this particular outcome\n",
    "            outc_labels_true = y_true[:, outc_id] == 1\n",
    "            output_dic[score_name].append(scoring_fn(outc_labels_true.astype(int), scores))\n",
    "\n",
    "    # Return object\n",
    "    return output_dic\n",
    "\n",
    "\n",
    "def compute_cluster_performance(X, clus_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute cluster performance metrics given input data X and predicted cluster probability assignments clus_pred.\n",
    "    Metrics computed include a) Silhouette Score, b) Davies Bouldin Score, c) Calinski Harabasz Score and d) Purity.\n",
    "    Performance is computed averaged over features.\n",
    "\n",
    "    Params:\n",
    "    - X: array-like of shape (N, T, D_f) where N is the number of patients, T the number of time steps and D_f the\n",
    "    number of features (+2, the id col and the time col).\n",
    "    - clus_pred: array-like of shape (N, ) with predicted label assignments.\n",
    "    - y_true: array-like of shape (N, num_outcs) with one-hot encoding of true class assignments.\n",
    "\n",
    "    Returns:\n",
    "        - Dictionary of output cluster performance metrics. Includes:\n",
    "            - \"Silhouette\": Silhouette Score computation.\n",
    "            - \"DBI\": Davies-Bouldin Index computation.\n",
    "            - \"VRI\": Variance-Ratio Criterion (also known as Calinski Harabasz Index).\n",
    "            - \"Purity\": Purity Score computation.\n",
    "    \"\"\"\n",
    "\n",
    "    # If not converted to categorical, then convert\n",
    "    if len(clus_pred.shape) == 2:\n",
    "        clus_pred = np.argmax(clus_pred, axis=1)\n",
    "\n",
    "    # Compute the same taking average over each feature dimension\n",
    "    sil_avg, dbi_avg, vri_avg = 0, 0, 0\n",
    "\n",
    "    for feat in range(X.shape[-1]):\n",
    "        sil_avg += silhouette_score(X[:, :, feat], clus_pred, metric=\"euclidean\")\n",
    "        dbi_avg += davies_bouldin_score(X[:, :, feat], clus_pred)\n",
    "        vri_avg += calinski_harabasz_score(X[:, :, feat], clus_pred)\n",
    "\n",
    "    # Compute Purity Score\n",
    "    purity_score = purity(y_true, clus_pred)\n",
    "\n",
    "    # Compute average factor\n",
    "    num_feats = X.shape[-1]\n",
    "\n",
    "    # Return Dictionary\n",
    "    clus_perf_dic = {\n",
    "        \"Silhouette\": sil_avg / num_feats,\n",
    "        \"DBI\": dbi_avg / num_feats,\n",
    "        \"VRI\": vri_avg / num_feats,\n",
    "        \"Purity\": purity_score / num_feats\n",
    "    }\n",
    "\n",
    "    return clus_perf_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from csv import writer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate(y_true=None, y_pred=None, clus_pred=None, data_info=None, save_fd=None, avg=None, scores=None,\n",
    "             **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluate function to print result information given results and/or experiment ids. Returns a dictionary of scores\n",
    "    given the outputs of the model (for e.g. if the model does not do clustering, then no cluster scores are returned).\n",
    "\n",
    "    Params:\n",
    "    - y_true: array-like of shape (N, num_outcs) with one-hot encodings of true class membership. defaults to None.\n",
    "    - y_pred: array-like of shape (N, num_outcs) with predicted outcome likelihood assignments. defaults to None.\n",
    "    - clus_pred: array-like of shape (N, num_clus) with predicted cluster membership probabilities. default to None\n",
    "    - data_info: dict of input data information and objects.\n",
    "    - save_fd: str, folder where to write scores to.\n",
    "    - age: str, useful how to average class individual scores (defaults to None, which returns no average).\n",
    "    - scores: array-like of shape (N, ) of scores. Only relevant for score-based benchmarks, such as NEWS2 and/or ESI.\n",
    "    - **kwargs: other parameters given to scoring supervised scores.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        - list of supervised performance scores and cluster performance scores (where valid).\n",
    "        - prints information for each of the associated score.\n",
    "        - saves score information in relevant folder.\n",
    "    \"\"\"\n",
    "    # Checks for instances Df vs array and loads data properties\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "\n",
    "    if \"news\" in save_fd.lower() or \"esi\" in save_fd.lower():\n",
    "\n",
    "        # Compute scores\n",
    "        scores = utils.compute_from_eas_scores(y_true=y_true, scores=scores, **kwargs)\n",
    "\n",
    "        # Definitions for completeness\n",
    "        cm = None\n",
    "        clus_metrics = {}\n",
    "\n",
    "    else:\n",
    "\n",
    "        if isinstance(y_pred, pd.DataFrame):\n",
    "            y_pred = y_pred.values\n",
    "\n",
    "        # Load data relevant properties\n",
    "        data_properties = data_info[\"data_properties\"]\n",
    "        outc_names = data_properties[\"outc_names\"]\n",
    "\n",
    "        # Compute scores and confusion matrix\n",
    "        scores, cm, Roc_curves = utils.compute_supervised_scores(y_true, y_pred, avg=avg, outc_names=outc_names)\n",
    "\n",
    "        # Convert Confusion Matrix to pdDataFrame\n",
    "        cm = pd.DataFrame(cm, index=pd.Index(data=outc_names, name=\"True Class\"),\n",
    "                          columns=pd.Index(data=outc_names, name=\"Predicted Class\"))\n",
    "\n",
    "        # If clustering results exist, output cluster performance scores\n",
    "        clus_metrics = {}\n",
    "        if clus_pred is not None:\n",
    "\n",
    "            if isinstance(clus_pred, pd.DataFrame):\n",
    "                clus_pred = clus_pred.values\n",
    "\n",
    "            # Compute X_test in 3 dimensional format\n",
    "            min_, max_ = data_properties[\"norm_min\"], data_properties[\"norm_max\"]\n",
    "            x_test_3d = data_info[\"X\"][-1] * (max_ - min_) + min_\n",
    "\n",
    "            # Compute metrics\n",
    "            try:\n",
    "                clus_metrics = utils.compute_cluster_performance(x_test_3d, clus_pred=clus_pred, y_true=y_true)\n",
    "            except ValueError:\n",
    "                print(\"Too little predicted labels. Can't compute clustering metrics.\")\n",
    "                clus_metrics = {}\n",
    "\n",
    "        # Save Confusion matrix\n",
    "        cm.to_csv(save_fd + \"confusion_matrix.csv\", index=True, header=True)\n",
    "\n",
    "    # Jointly compute scores\n",
    "    scores = {**scores, **clus_metrics}\n",
    "\n",
    "    # Save\n",
    "    # for key, value in Roc_curves.items():\n",
    "    \n",
    "    #     # Get fig, ax and save\n",
    "    #     fig, _ = value\n",
    "    #     fig.savefig(save_fd + key)\n",
    "\n",
    "\n",
    "    with open(save_fd + \"scores.csv\", \"w+\", newline=\"\\n\") as f:\n",
    "        csv_writer = writer(f, delimiter=\",\")\n",
    "\n",
    "        # Iterate through score key and score value(s)\n",
    "        for key, value in scores.items():\n",
    "\n",
    "            # Define row to save\n",
    "            if isinstance(value, list):\n",
    "                row = tuple([key, *value])\n",
    "            else:\n",
    "                row = tuple([key, value])\n",
    "\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "    # Print information\n",
    "    print(\"\\nScoring information for this experiment\\n\")\n",
    "    for key, value in scores.items():\n",
    "        print(f\"{key} value: {value}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix for predicting results\", cm, sep=\"\\n\")\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db742e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
